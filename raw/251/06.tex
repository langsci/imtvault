\chapter{Phoneme split}\label{ch:Phoneme-Split}

In this chapter I will develop a model of \isi{phoneme split}, or genesis,
using the phenomenon of \isi{vowel nasalization} as a case study. The model
will be based on the analyses of the preceding chapters: the metric
of success will be representational consistency and stability, with
the ability to achieve multiple stable states under different parameter
settings. The relevant parameters will also be required to serve as
testable hypotheses about possible \isi{actuation} mechanisms. The first
of two model variants, the No-Phoneme Model, will contain explicit
representations only at the word level, and will be used primarily
to illustrate a particular implementation of the \isi{frequency} effect.
The subsequent model, the Multiple-Parse Model, will add a \isi{sub-lexical}
level of analysis. The major innovation of this model will be an explicit,
non-one-to-one, perception-to-\isi{production} mapping in which the likelihood
of a given analysis depends on the \isi{phonetic} properties of the input.
Additionally, no analysis is taken to be more ``correct'' than any other,
just as the set of possible \isi{sub-lexical} units is not taken to be determined
ahead of time.

\section{Representations I}\largerpage
It has been well-established in both \isi{perception} and \isi{production} that
there is a negative correlation between degree of \isi{vowel nasalization} and strength
of nasal consonant (e.g. \citealt{kawasaki1978perceived,cohn1990phonetic}).
This is consistent with the hypothesis that the final nasal is more
likely to be lost, the more nasalized the preceding vowel becomes.
A possible explanation can be found in a listener-oriented theory
of change, where speakers strive to preserve acoustic cues for ease
of listener comprehension. Strong nasal cues on the vowel predict
the upcoming nasal, which means that speakers may expend less effort
to preserve the actual nasal, allowing it to erode. As with other
proposals, the question that still remains to be answered is how the
vowel came to have such strong nasal cues in the first place (presumably
stronger than the typical range of \isi{phonetic} \isi{nasalization} observed
cross-linguistically). 

A different perspective will be adopted here, building on the observation
of \citet{Beddor2009} that the negative correlation between vowel
\isi{nasality} and consonant \isi{nasality} follows directly from a single \isi{articulatory}
parameter: the degree of overlap of the vowel and nasal gestures.
The more overlap, the greater the extent of \isi{nasalization} on the vowel,
and the shorter the duration of the purely consonantal nasal, and
vice versa (see \figref{fig:Normal nasalization}). It will be assumed
that successful \isi{production} requires stored \isi{articulatory} targets, and
that these must be inferred from acoustic inputs. For simplicity,
only a single word type will be modeled, that consisting of a tongue
body gesture followed by a \isi{velum} gesture (e.g. \textit{am}). The production-\isi{perception}
mapping will take place over three \isi{phonetic} variables: duration of
\isi{tongue body} gesture, duration of \isi{velum} gesture, and duration of \isi{gestural}
overlap $(x^{V},x^{N},x^{O})$. Categorization will occur at the level
of the word, and does not require decomposition into phonemes. Thus,
these models will not assume that there exists an \isi{allophonic} process
of \isi{vowel nasalization}. The initial distributions for all tokens will
be generated by independent sampling from three separate normal distributions,
corresponding to $x^{V}$, $x^{N}$, and $x^{O}$, respectively. 

In \chapref{ch:Models-of-Change} we saw that (soft) targets\largerpage
were needed to constrain the basic \isi{exemplar} model of change. What
this did was effectively force an independent \isi{production} component
into a model which otherwise equated acoustic and \isi{articulatory} representations.
In the current set of models this is not necessary, implementationally,
or conceptually, because each acoustic token has its own \isi{production}
\isi{target}. This is depicted schematically in \figref{fig:P-toP-mapping},
where stored \isi{articulatory} parameters (represented by temporally overlapping
\isi{articulatory} gestures) are realized as acoustic tokens during \isi{production}
(dark patterned rectangles representing sound \isi{frequency} information
over time), and acoustic tokens are, in their turn, transformed into
stored \isi{articulatory} parameters during \isi{perception}. At the word level,
this is a \noun{state} model; the \isi{articulatory} variables are stored
without \isi{normalization}. At the \isi{gestural} level, however, there is the
possibility of an implicit \noun{process} model in the fact that the
overlap dimension represents the concatenation of two units, as well
as the source of \isi{nasalization}. I will return to this point below.

\begin{figure}[h]
\includegraphics[width=0.45\textwidth]{figures/SeparateReps.pdf}\caption{\label{fig:P-toP-mapping}Graphical depiction of an explicit mapping
between articulatory representations and acoustic representations}
\end{figure}

As \sectref{sec:Model-Behavior} showed, the two-attractor (\noun{state}
model) had a limited range of output states: the entire distribution
always stabilizes at some point intermediate between the two attractors.
Furthermore, that model contained no mechanism for changing the attractor
locations, or introducing new attractors. The current model effectively
explodes the number of attractors (or underlying representations)
to the number of tokens within a category. This allows for more complex
model dynamics. It also allows for changes to occur to the attractors
themselves, via independent forces that act, not at the level of the
word (or at the level of the \isi{phoneme}), but at the level of the \isi{gestural}
variables. Individual tokens of a given word category can thus be
altered, with the possibility, but not the guarantee, that such changes
can spread throughout the entire distribution of tokens.

\section{\label{sec:Frequency-I}Frequency I}

The first iteration of the phoneme-split model uses a frequency-based
attractor as the actuator of change. Based on the assumption that
frequency-based reduction is the result of increased \isi{fluency}, and
that to be fluent is to produce some (nearly) ideal balance of efficiency
and intelligibility, an optimal degree of \isi{gestural} overlap, \emph{T},
is defined. Relative word \isi{frequency} is implemented as a parameter
($\beta$; $0<\beta<1$) that controls the degree to which each token
of the category is shifted towards the \isi{target}, \emph{T}, during \isi{production}.
Thus, for a stored \isi{production} token consisting of the duration triple
$(x_{i}^{V},x_{i}^{N},x_{i}^{O})$, a \isi{fluency} effect applies to the
ultimate realization of $x_{i}^{O}$, the \isi{gestural} overlap value,
in the following way: 
\begin{equation}
\textit{Fluency}:x_{i}^{O^{\prime}}=x_{i}^{O}+\beta(T-x_{i}^{O})\label{eq:Frequency attractor}
\end{equation}
Because the absolute duration of the optimal \isi{gestural} overlap will
depend on the durations of the gestures for each specific token, $T$
is expressed as a function of $x_{i}^{N}$. In the case where $T=x_{i}^{N}$,
the \isi{fluency} pressure always acts to increase $x_{i}^{O}$ (as long
as $x_{i}^{O}\neq x_{i}^{N}$), because the duration of overlap can
never be larger than the duration of the nasal gesture (assuming also
that $x_{i}^{V}$ is always greater than $x_{i}^{N}$). 

\section{Speaking rate}

As has already been demonstrated several times, a single attractor
results in a single possible outcome. With only the \isi{frequency} attractor
acting on productions, maximal \isi{fluency} is the only possible outcome,
regardless of the value of $\beta$. Implementationally, a force is
needed to counter-act the \isi{fluency} effect. For a \isi{fluency} \isi{target} at
maximal overlap, that opposing force must act to decrease overlap.
However, in the general case, it may be desirable to include a force
that can either increase or decrease the relevant parameter values.
In fact, regardless of the implementational requirements for a successful
model, there are clear theoretical reasons to include a bi-directional
force affecting \isi{articulatory} durations. 

Changes in \isi{speaking rate}, of course, strongly influence the absolute\largerpage
duration of speech sounds. Furthermore, there are similarities between
the reduction effects observed in fast speech, and those observed
with high-\isi{frequency} words. Therefore, whatever drives changes in speaking
rate is clearly relevant in a model of change in which duration plays
a role. Equally important is the fact that changes in \isi{speaking rate}
are not all increases in speed, and the effect of slowed speech in
potentially disrupting cumulative change cannot be selectively ignored. 

Changes in \isi{speaking rate} have been shown to affect both the absolute
duration as well as the timing between sequential speech units (\citealt{stetson1928motor,Hardcastle1985}),
and therefore are taken to affect all of $(x^{V},x^{N},x^{O})$ in
the phoneme-split model. I will adopt the view here that changes in
\isi{speaking rate} are governed by forces largely external to the mechanisms
of sound change, and that changes in rate can therefore be modeled
as a stochastic process.\footnote{There is a strand of research that assumes that changes in speaking
rate, specifically decreases in \isi{speaking rate}, are driven by a desire
to enhance or exaggerate a given \isi{phonological} contrast (e.g. \citealt{beckman2011rate}).
Although slowed \isi{speaking rate} often occurs under conditions in which
speakers are deliberately hyper-articulating their speech, I assume
that decreases in \isi{speaking rate} can also occur independently; that
is, that speakers can control their rate of speech, e.g. when asked
to match the beat of a metronome, without consciously trying to produce
more intelligible speech.}

Changes in \isi{speaking rate}, affecting word duration, are modeled in
the following way. At \isi{production}, a value is randomly selected from
a normal distribution centered about 0. This value represents the
force (\emph{E}) that will act on that token: either to expand it
(if positive), or to compress it (if negative). Expansion results
in longer words, corresponding to slower speaking rates, and compression
results in shorter words, corresponding to faster speaking rates.
Each \isi{articulatory} parameter is independently subjected to this force.
The degree to which a given gesture is actually expanded or compressed
depends on how inherently elastic it is. This elasticity is implemented
as a parameter that controls the steepness of a logistic curve. For
example, the effect of force \emph{E} acting on the overlap variable
($x^{O}$) is given in Equation (\ref{eq:Speaking rate transform}): 
\begin{equation}
\textit{SpeakingRate}:x_{i}^{O^{\prime\prime}}=\frac{A}{(1+e^{kE})}\label{eq:Speaking rate transform}
\end{equation}
\emph{A} is a \isi{normalization} factor, and is set to $2x_{i}^{Z^{\prime}}$
for all variables (\emph{Z}). This has the effect of making the adjusted
\isi{length} depend on the current \isi{length}, with $E=0$ resulting in no change.
Note that for decreases in \isi{speaking rate}, overlap should decrease
– pulling the two gestures apart, and thus \isi{lengthening} the word –, while for increases in \isi{speaking rate}, overlap should increase. Therefore,
the dependence of overlap on expansion degree is expressed as a positive
exponential, while the dependence of the other two duration parameters
is expressed as a negative exponential. For these simulations all
three \isi{articulatory} variables were set to the same elasticity ($k=1$). 

\section{\label{sec:No-Phoneme-Phoneme-Split-Model}No-Phoneme Phoneme-Split Model}\largerpage

In order to understand the behavior of the phoneme-split models, we
first create a version with only the \isi{speaking rate} mechanism included.
Figure \ref{fig:SpeakingRateOnly} shows the outcome that the speaking
rate distribution (\emph{E} distribution) selects for, given a particular
starting distribution of tokens. The three \isi{articulatory} parameters
are plotted separately, in different colors. This model also assumes
error-free mapping of acoustics to articulation, outside of a small
error term in \isi{production}. Entrenchment and \isi{memory decay} apply as in
previous models.

\begin{figure}[h]
\includegraphics[width=0.6\textwidth]{figures/SpeakingRateI10000.pdf}\caption{\label{fig:SpeakingRateOnly}Phoneme-Split Model: Speaking rate only}
\end{figure}

With appropriately chosen constants, the \isi{speaking rate} force is capable
of disrupting the influence of the frequency-based attractor. This
allows the \isi{equilibrium} state of the model to vary as a function of
word \isi{frequency} ($\beta$). For $T=x_{i}^{N}$, the \isi{fluency} effect
acts consistently to increase \isi{gestural} overlap ($T=x_{i}^{N}$). If
it is too weak ($\beta$ too small), then the \isi{speaking rate} \isi{equilibrium}
shown in Figure \ref{fig:SpeakingRateOnly} prevails. If it is strong
enough, then it is able to shift the overlap distribution to larger
values (rightward). This is possible because the \isi{speaking rate} transform
depends on the current value of the overlap parameter for any given
token (expressed in the variable \emph{A} of Eq. \ref{eq:Speaking rate transform}).
Figure \ref{fig:NasalizationModel1} shows the output of the model
with both \isi{speaking rate} and \isi{frequency} \isi{bias}, run for three different
values of $\beta$. Note that the number of model iterations is essentially
arbitrary. Because the number is large (10,000), there is a reasonable
expectation that a stable state has been reached, but no tests of
convergence were performed. In this section I am more concerned
with the qualitative behavior of the model, and comparisons in which
all but one aspect of the simulations are kept constant.

\begin{figure}[h]
\begin{subfigure}[t]{.3\textwidth}
        \includegraphics[width=\linewidth]{figures/SpeakingRateFrequencybeta05.pdf}
        \caption{Low-frequency word ($\beta=0.05$)}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{.3\textwidth}
        \includegraphics[width=\linewidth]{figures/SpeakingRateFrequencybeta2.pdf}
        \caption{Mid-frequency word ($\beta=0.2$)}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{.3\textwidth}
        \includegraphics[width=\linewidth]{figures/SpeakingRateFrequencybeta5.pdf}
        \caption{High-frequency word ($\beta=0.5$)}
    \end{subfigure}

% % % \subfloat[Low-\isi{frequency} word ($\beta=0.05$)]{\includegraphics[width=0.25\textwidth]{figures/SpeakingRateFrequencybeta05.pdf}}
% % % \subfloat[Mid-\isi{frequency} word ($\beta=0.2$)]{\includegraphics[width=0.25\textwidth]{figures/SpeakingRateFrequencybeta2.pdf}}
% % % \subfloat[High-\isi{frequency} word ($\beta=0.5$)]{\includegraphics[width=0.25\textwidth]{figures/SpeakingRateFrequencybeta5.pdf}}
\caption{\label{fig:NasalizationModel1}No-Phoneme Phoneme-Split Model (Frequency Attractor). Each model run for 10,000 cycles with identical starting conditions.}\is{Attractor}\end{figure}

The No-Phoneme Phoneme-Split model predicts that words with higher
frequencies should be produced, on average, with vowels that are more
nasalized (larger degree of overlap between gestures), than lower
\isi{frequency} words. It also shows that it is possible to achieve stable
\isi{phonetic} change from a change in word \isi{frequency}. The model implements
a theory of nasal vowel genesis as an emergent property of gradient
effects acting directly on \isi{articulatory} parameters. Only in the special
case where overlap is roughly equivalent to nasal duration, would
the data likely be analyzed (by a linguist) as the result of \isi{phoneme}
split. This state in the model, however, has no special status. And
distributions that appear intermediate with regard to the average
ratio of overlap duration to \isi{velum} gesture duration can be stable.
Conceptualizing \isi{phoneme split} in this way allows us to avoid the \isi{actuation}
paradox that requires the loss of the conditioning environment, but
the retention of the conditioned \isi{allophone} (see \sectref{sec:Actuation-1}).
In fact, as there are no phoneme-level representations in this model,
there are no allophones, and no conditioning environments, in the
classical sense. Therefore, this model is also a demonstration that
phoneme-level representations are not necessary (nor is any \isi{misperception}/\isi{misarticulation}
pressure of the kind discussed in the previous chapter), to achieve
a working model with the generally correct behavior. 

The source of the \isi{nasalization} effect in this model is the coordination
between the \isi{tongue body} and \isi{velum} gestures. This parameter, however,
is part of the underlying specification of each word token. The No-Phoneme
model is thus a \noun{state} model. Arguably, a \noun{state} model
represents a change that has already taken place, in which a process
of \isi{nasalization} has been reinterpreted as a static property of a unitary
representation. In the next sections I will turn to a \noun{process}
model of \isi{vowel nasalization}, returning to the \isi{misperception}/\isi{misarticulation}
\isi{actuation} mechanism. This will also involve introducing a \isi{sub-lexical}
level of representation, and revisiting the implementation of word
\isi{frequency}.

\section{Parsing and misparsing}

In order to recover the meaning of a given speech signal, it is necessary,
at minimum, to identify the individual lexical items present. This,
in turn, requires determining where one word ends and the next begins.
The highly context-sensitive nature of acoustic cues, as well as the
lack of consistent silence, or other markers, of the boundaries between
words or sounds, make this a computationally difficult task. And this
is not just an acquisition problem. Signal \isi{parsing}, or segmentation,
is something that must be carried out every time speech is perceived. 

That segmentation of some kind must also take place at the \isi{sub-lexical}
level is evidenced by a large literature on what are known as “trading
relations”, in which the value along a given \isi{phonetic} dimension
that separates two members of a phonemic contrast is shown to vary
depending on the values of the other \isi{phonetic} cues present. And those
other cues that influence the boundary location are not just those
that occur within the segment itself. For example, a given phone ({[t]})
may be ambiguous as to whether it belongs with a preceding or following
word (e.g. \textit{great ship} {[}{ɡɹe͡ɪt}\#{ʃɪp}{]}
versus \textit{gray chip} {[}{ɡɹe͡ɪ}\#{t͡ʃɪp}{]}),
and the actual word sequence that is heard will depend on the durations
of the surrounding segments; longer durations of {[e͡ɪ]}
increase the likelihood of \textit{gray} over \textit{great}, while
shorter durations of {[ʃ]} increase the likelihood of \textit{chip}
over \textit{ship}. An acoustic cue (such as silence itself) may also
be ambiguous as to whether it originates from a \isi{phoneme} ({/t/})
or a break between words (\textit{great ship} versus \textit{gray ship}
{[}{ɡɹe͡ɪ}\#{ʃɪp}{]}; longer durations of silence
increase the likelihood of \textit{great} over \textit{gray} (\citealt{repp1978perceptual}).
An acoustic feature may also be ambiguous as to whether it belongs
to a preceding segment, a following segment, or both ({[}{ɹa͡ɪpbɛɹiz}{]}
as either \textit{right berries} or \textit{ripe berries}, \citealt{Gow2003}). 

In all of the preceding examples the ambiguity exists because of the
existence of multiple real-word alternatives. Without those alternatives,
or competitors, phonetically ambiguous input quickly becomes perceptually
unambiguous (e.g. \citealp{warren1970perceptual,ganong1980phonetic}).
The strong susceptibility of low-level \isi{perception} to high-level expectations
also speaks to the amount of noise, or essentially unpredictable variability,
in the acoustic realization of a given abstract category. Speech \isi{perception}
involves the complex integration of multiple cues, each of which,
in isolation, may be relatively uninformative, in order to arrive
at a single \isi{parse}, a single percept, of what is heard. This percept
is presumably the best alternative among those available to the listener
(see \citealt{davis2007hearing} for a review of the literature). Although
speech \isi{perception} appears extremely robust due to the fact that the
meaning intended by the speaker is usually recoverable by the listener,
that robustness is a property of the entire set of cues available,
not of acoustic features alone, and certainly not of individual acoustic
features. Rather than conceptualizing sound change as the relatively
rare event in which the listener mishears, or the speaker misspeaks,
it may be the case that what we typically think of as the “changed”
variants are already present within the distribution of stored tokens,
as one of multiple possible parses of each inherently ambiguous input
signal.

\section{Multiple parses}

The classical way in which sound change is conceptualized is based
on the assumption that there exists a unique, correct, \isi{sub-lexical}
representation for each word. It is meaningless to speak of phoneme-level
“errors” unless this is the case. Consider the following hypothetical
example (where \emph{$x>y$} indicates an historical change from \emph{x}
to \emph{y}):
\begin{covexamples}
\item \label{anpa>ampa}\emph{anpa \textgreater{} ampa}
\end{covexamples}
(\ref{anpa>ampa}) is a common type of change known as nasal place
assimilation. In this example, the coronal feature of the nasal $/n/$
is assimilated to (or replaced by) the labial feature of the following
$/p/$. Speakers of a language that undergoes this change presumably
had an earlier \isi{allophonic} rule specifying that $/n/$'s preceding
stops take on the place features of that stop. Therefore, the change
could only have occurred if they uncharacteristically failed to account
for this rule, or they made the “wrong” choice for a \isi{production}
that was especially strongly assimilated. In either case, listeners
are assumed to \isi{parse} their acoustic input into a sequence of discrete
phones, deciding for each segment whether to \isi{normalize} or accept at
face value. Thus, for a change from \emph{/anpa/} to \emph{/ampa/}
to have actually occurred in the way it is denoted here, it must be
the case that listeners used to routinely segment continuous acoustic
tokens of this word into the sequence of units \emph{/a/}, \emph{/n/},
\emph{/p/}, \emph{/a/}, until they switched to segmenting
those tokens into the sequence \emph{/a/}, \emph{/m/}, \emph{/p/},
\emph{/a/}. 

Of course, we know that a discrete series of abstract symbols (either
\emph{[anpa]} or \emph{[ampa]}) is not present in the acoustic
signal in any objective sense. The abstract notation also implies
that this change occurs once, simultaneously, for all words, and for
all word tokens. However, adopting the hypothesis that multiple experienced
instances of speech are stored implies that change would have to occur
over individual tokens. In fact, the multiple-\isi{parse} hypothesis is
a logical consequence of the basic tenet of the \isi{exemplar} framework.
The conflation of \isi{perception} and \isi{production} that we saw in the \isi{exemplar} models of Chapters~\ref{ch:The-Exemplar-Model} and~\ref{ch:Models-of-Change}
is borrowed directly from the standard generative notation. Once a
transformation from perceptual tokens to \isi{production} tokens is required,
it becomes clear that 1) \isi{parsing} is necessary in the first place,
and 2) it must occur for each experienced token. Recognizing
that acoustic tokens are inherently ambiguous with respect to their
decomposition into discrete units suggests, in turn, that variable
parses might be the norm rather than the exception.\footnote{This is closely related to the proposal that stored lexical items
can have more than one representation (see e.g. \citealp{hooper1976word,Janda2008,Bybee2001}).
Split representations are also assumed to be the outcome of discontinuous
\isi{articulatory} change in the model of \citet{Garrett2013}.} 

In the nasal assimilation example, there are two obvious alternative
parses, differing in whether they contain the \isi{phoneme} $/n/$ or $/m/$,
thus the word-level category \textit{anpa} is hypothesized to be composed
of at least some tokens specified with \isi{production} targets for $/n/$,
and some for $/m/$. However, additional possible parses exist if
we do not assume the available \isi{phoneme} inventory \emph{a priori}.
In fact, if we allow all universally possible segments into the analysis
space, then we avoid the \isi{actuation} paradox of the classical diachronic
approach. As the next section will show, this re-framing of the change
question allows \isi{synchronic variation} to be linked to diachronic change
in a way that is not dependent on either stopping or starting the
model at a critical point in time. 

\section{Representations II}

The Multiple-Parse model adds a \noun{process} component to the No-Phoneme
model. The process is implemented at the level of the \isi{articulatory}
gesture, but conceptually requires the existence of abstract categories
intermediate between the word and the gesture. As before, the change
occurs in the distribution of variants that already exist, rather
than in the genesis of entirely novel forms. This aspect bears some
similarity to the proposal in \citet{Baker2011}, based on misanalysis
of the signal, but the current model is not abrupt, nor does it require
“extreme” variants to be adopted.

The conversion from \isi{perception} to \isi{production} is the locus of \isi{sub-lexical}
\isi{parsing}, mapping every continuous acoustic token into a series of
categorical units. In principle, these units can consist of any contiguous
set as long as it is phonetically plausible, and exhaustively parses
the input signal. However, in the case of \isi{vowel nasalization}, I will
be concerned with two particular possibilities: the one-sublexical-unit
analysis, and the two-sublexical-unit analysis. These are of special
interest, of course, because they bear considerable similarity to
the classical analyses of the phenomenon before change (two units),
and after change (one unit). However, it is important to be very careful
in how these units are described, because the traditional notational
system essentially forces an analysis more general than the
word level. In order not to assume generalization, and remain representationally
consistent, the following notation will be adopted for the two \isi{sub-lexical}
parses of the word in question (\textit{am}): $/\tilde{V}_{am}/$ (Analysis
1), and $/V_{am}/+/N_{am}/$ (Analysis 2). The desired implication
is that only after generalization across multiple words could something
similar to the abstract categories $/\widetilde{V}/$ and $/V/+/N/$
arise.

For the \isi{articulatory} parameters already defined, a single-unit \isi{parse}
means that all three values will be stored on the \isi{production} side.
$/\tilde{V}_{am}/$ is a 3-dimensional cloud, and \isi{entrenchment} applies
over each dimension. Note that this is what was assumed for all tokens
in the No-Phoneme Model, which therefore implicitly assumed a single-unit
analysis. The two-unit \isi{parse}, however, is explicitly a \noun{process}
analysis, entailing that one token is drawn from a one-dimensional
\emph{$/V_{am}/$} cloud, one from a one-dimensional \emph{$/N_{am}/$}
cloud, with concatenation occurring at the time of \isi{production}. In
other words, the overlap between the two gestures is not stored, but
determined online. 

\section{Multiple-Parse Phoneme-Split Model}

Either analysis is possible for any given token, but, critically,
depends on the acoustic properties of that token. In this set of simulations
it will be assumed that word-level categorization is correct, and
that the three duration quantities ($x_{i}^{V},x_{i}^{N},x_{i}^{O}$)
are accurately recovered in \isi{perception}, although this is not critical.\footnote{If the error term is symmetrical, then it will have no qualitative
effect on the model dynamics.} Analysis 1, the single-segment analysis, is more likely to be selected,
the more highly overlapped the gestures that produced that token,
while Analysis 2, the 2-segments-in-sequence analysis, is more likely
for less overlapped gestures. The specific dependence is on the quantity
$Q_{i}={x_{i}^{O}}/{x_{i}^{N}}$. Larger values of $x_{i}^{O}$
lead to larger values of $Q_{i}$, as do smaller values of $x_{i}^{N}$.
Selecting for large \emph{Q} thus selects both for larger overlap
and shorter word durations. That duration should correlate with number
of constituents is a reasonable hypothesis. It can also be hypothesized
that \isi{articulatory} gestures will tend to be more tightly coordinated
within, than across, segments, if shared constituency promotes greater
merger.\footnote{I am not aware of evidence for this specific relationship, but there
is evidence for different types of \isi{gestural} coordination across different
domains: between the onset and nucleus of a syllable, versus the nucleus
and coda (\citealt{Browman1988,byrd1996influences}); and within,
versus across, morpheme boundaries (\citealt{Cho2001}).} The probability of Analysis 1, $P(a=1)$, depends on \emph{Q} in
the following way (\ref{eq:segmentation-1}).
\begin{equation}
P(a=1)=Ae^{-b(1-Q)}-C\label{eq:segmentation-1}
\end{equation}
Probability increases with increasing \emph{Q} because of the negative
exponential in (\ref{eq:segmentation-1}). The largest possible value
for \emph{Q} is 1, therefore $1-Q$ is always positive. When $Q=1$
, $P(a=1)$ reaches its maximum at $A-C$. How quickly the probability
decreases as a function of decreasing \emph{Q} is controlled by the
variable \emph{b. }The larger \emph{b}, the larger the negative exponential,
and the more quickly $P(a=1)$ decreases, selecting for larger mean
\emph{Q} values (and fewer tokens). See Appendix \ref{chap:Appendix E}
for additional details.

If Analysis 1 is chosen in \isi{perception}, based on the value of $P(a=1)$,
then all three values of the token are stored. If Analysis 2 is chosen,
then the duration of the \isi{tongue body} gesture ($x_{i}^{V}$), and the
duration of the \isi{velum} gesture ($x_{i}^{N}$), are each stored in separate
categories, and the overlap value is discarded. Figure \ref{fig:MultiParse-Reps}
provides a schematic depiction of these relationships. Note that the
dimensions are not accurately represented here; two dimensions are
used for all categories to make the membership relationships easier
to see. Individual tokens are drawn as schematic \isi{gestural} scores:
extent represents time, and fill type represents active \isi{articulator}.
Where the two bars appear together, horizontal alignment indicates the stored \isi{gestural} overlap parameter. The thin
lines drawn between tokens of different sub-categories indicate that
they are stored together, and will be produced together. However, overlap must
be determined separately because it is not stored as part of the experienced token. Entrenchment happens only
within individual \isi{sub-lexical} categories.\footnote{If \isi{entrenchment} at the word level is added it will have the effect
of pushing values back towards the means of the Analysis 2 categories,
since the model is initialized with those values, and the Analysis 2
\isi{parse} is always more likely.}

\begin{figure}[h]
\includegraphics[width=0.6\textwidth]{figures/MultiParseModel.pdf}\caption{\label{fig:MultiParse-Reps}Schematic depiction of the relationships
between the word-level category (\textit{am}) and the sub-lexical level
categories of its constituents. Production-side representations.}
\end{figure}

Tokens are chosen randomly for \isi{production} from among all stored values.
Once selected, the token is subject to the same \isi{speaking rate} transformation
used in the No-Phoneme model. The overlap degree for an Analysis-2
token defaults to a fixed percentage of the current average value
of $x^{N}$ (with some variance). There is no \isi{phonetic} \isi{bias} in this
model. The selection \isi{bias} that drives the feedback loop resides in
the choice of underlying analysis – the \isi{parsing} of the input signal.
Equation~\ref{eq:segmentation-1} selects for large values of $x^{O}$,
and for small values of $x^{N}$, both of which will increase the
size of \emph{Q}, and thus increase the probability of Analysis~1.
There is no cumulativity such that these values grow more extreme,
but there is a constant pressure to sort tokens with the largest overlap
proportion into this \isi{sub-lexical} category. Thus, if an independent
mechanism resulted in an increase in \emph{Q} for some tokens, those
tokens would raise the average \emph{Q} value of the $/\tilde{V}_{am}/$
sub-category.

A mechanism that shortens word duration will have this effect: shortening
$x^{N}$ (and $x^{V}$), and \isi{lengthening} $x^{O}$. If higher \isi{frequency}
is taken to result in faster productions, then an increase in \isi{frequency}
will lead both to more $/\tilde{V}_{am}/$ categorizations, and to a higher \emph{Q} value for the category on average. For the simulations
reported below, \isi{frequency} was implemented as a negative perturbation
to the mean value of the expansion force distribution. As a result,
higher \isi{frequency} (higher \isi{resting activation}) results in shorter words,
and thus shorter \isi{tongue body} and \isi{velum} gestures (and longer overlap)
under all speaking rates. This implementation will be discussed in
more detail in the following section. 

Figure \ref{fig:Multiple-Parse-Results} shows the model results as
a function of \isi{frequency}. Each point is the result of running the model
for 10,000 iterations. Mean values for the three duration parameters,
as well as the proportion overlap (\emph{Q}) are given for each of
the categories - Panel 1: word-level; Panel 2: Analysis 1 tokens; Panel
3: Analysis 2 tokens. Note that the overlap proportion in Panel 3
shows the constraint that overlap proportion stay fixed with respect
to $\overline{x^{N}}$. Whereas, in Panel 2, as \isi{resting activation}
(\isi{frequency}) increases, the proportion overlap increases. Because the
number of tokens parsed into the $/\tilde{V}_{am}/$ category also
increases with increasing \isi{resting activation} (from approximately 31\%
to 50\%), the overlap proportion increases for the word-level category
as a whole (Panel 1). 

\begin{figure}[h]
\includegraphics[width=.70\textwidth]{figures/MultipleParseResults.pdf}
\caption{\label{fig:Multiple-Parse-Results}Multiple-Parse Model: Results as
a function of resting activation (word frequency). Each point corresponds
to the mean after 10,000 model iterations. Mean articulatory durations
are plotted in black. Proportion overlap ($Q={\overline{x^{O}}}/{\overline{x^{N}}}$)
is plotted in red. Panel 1: all word tokens; Panel 2: Analysis-1 tokens
only; Panel 3: Analysis-2 tokens only.}\is{articulatory}
\end{figure}


\section{\label{sec:Frequency-II}Frequency II}

In the No-Phoneme Phoneme-Split model, \isi{frequency} was implemented as
a fixed attractor on overlap duration (Section \ref{sec:Frequency-I}).
The assumption was that there existed an optimal (most fluent) \isi{production}
of a given word with precisely the degree of \isi{gestural} overlap given
by the attractor \isi{target}. At the same time, in order to generate productions
that more closely resembled nasal vowels, it was necessary to set
the \isi{target} quite high – in the reported simulations it was set to
the entire duration of the accompanying \isi{velum} gesture. However, it
is not clear why the optimal \isi{production} of the 2-gesture word should
exhibit such a large degree of overlap. And, in general, there is
no clear reason for greater practice, or increased \isi{fluency}, to always
result in shortened, or reduced, articulations, especially to the
point where distinctiveness may be lost at the word and/or \isi{phoneme}
level. Yet this seems to be the case with \isi{frequency} effects. It has
been shown, for example, that individual segments within high-\isi{frequency}
words are shorter, and that they are more likely to exhibit “deletions”
(dropping, or masking of a consonant, or unstressed vowel, e.g.
\citealt{Bell2003,Raymond2006,Bybee2008}). The realizations of segments
in higher-\isi{frequency} words tend also to be less extreme, or more “centralized”,
perhaps failing to reach the usual \isi{articulatory} \isi{target} (e.g. \citealt{munson2004effect,Scarborough2004,gahl2008time}).

The listener-based account of \isi{frequency} effects explains these phenomena
as a consequence of contextual predictability. It is actually the
less predictable, less easy to access, more confusable, forms that
are produced with particular care (hyper-articulated) by the speaker
in order to aid intelligibility (e.g. \citealt{Aylett2004}). In
the absence of that pressure, articulations are reduced to the degree
possible, facilitating the task of the speaker. Factors that have
been shown to affect predictability, as well as word form, include
sentence, or discourse, context, bigram \isi{frequency}, and unigram \isi{frequency},
among others. Nevertheless, there are a number of results that are
not compatible with a strictly listener-based theory, studies that
have shown that speakers do not always alter their productions in
such a way as to facilitate listener comprehension (see \citealt{turnbull2015assessing}
for a review of the literature).

As mentioned briefly in \sectref{subsec:Word-Frequency}, the
speaker-based approach attributes \isi{frequency} effects to automatic production-side
mechanisms. This is usually couched in terms of activation levels,
within some kind of lexical network model where different representations
“compete” in both \isi{perception} and \isi{production} (e.g. \citealp{mcclelland1981interactive,Dell1986}).
In terms of word retrieval, the successful candidate is the one that
achieves a given threshold of activation first. Every time a word
is accessed, or produced, it is activated to this level. Repeated
activations, within some time period, are taken to result in some
level of residual activation that persists even when the word is not
selected. This “resting” activation level is naturally higher
in higher \isi{frequency} words, giving them a head start against lower-\isi{frequency}
competitors. 

The resting-activation account is in line with results establishing
that higher-\isi{frequency} words are produced earlier than lower-\isi{frequency}
ones in a variety of tasks, such as picture naming, and word or sentence
reading – even with delays. Higher-\isi{frequency} words also lead to faster
response times in lexical decision and other speeded response tasks,
as well as to greater accuracy in word recognition (e.g. \citealt{howes1951visual,balota1985locus,Luce1986,Marslen-Wilson1990}).
However, it is not at all obvious that higher \isi{resting activation} alone
can account for \isi{articulatory} or temporal reduction (hypo-articulation).

In fact, it has been argued \emph{both} that a higher activation level
should lead to hyper-articulation (e.g. \citealt{Baese-Berk2009}),
and that it should lead to hypo-articulation (e.g. \citealt{gahl2012reduce}).\footnote{Note that different results were obtained in these studies, one based
on laboratory data, and one on conversational corpus data.} In works that adopt the latter position, the connection seems to
be assumed. For example, \citet[79]{gahl2012reduce}
write that “Production-based accounts ... would lead one to expect
that words that are retrieved quickly tend to be phonetically reduced
– \emph{provided that fast retrieval speed translates into fast \isi{production}
speed”} (emphasis mine).

The fact that there does not appear to be a well-worked out mechanism
for this result raises the possibility that we have yet to find the
right model for \isi{frequency}. Empirically, however, the correlation between
shorter/faster productions and higher word \isi{frequency} seems quite robust.
In the Multiple-Parse Model, a frequency-based increase in \isi{production}
speed is taken to be an additive effect, acting to effectively shift
the \isi{speaking rate} distribution. If I continue to assume that speaking
rate acts independently of other model forces, then words will continue
to be pulled in both directions, expanding or compressing in turn.
Subject to the same large positive force, both low and high \isi{frequency}
words will be produced more slowly, and will thus be longer than if
no force had applied. However, the high-\isi{frequency} word will be somewhat
shorter than its low \isi{frequency} counterpart, due to the difference
in its resting state. The same will be found under compression (unless
floor is reached).

The dependence on \isi{frequency} (\isi{resting activation}) in Figure \ref{fig:Multiple-Parse-Results}
shows the effect of progressively shifting the \isi{speaking rate} distribution;
shorter average word durations result in shorter $x^{N}$, and longer
$x^{O}$, and thus lead to a greater proportion of Analysis 1 tokens.\footnote{Because the duration of overlap cannot exceed the \isi{length} of the shortest
gesture, the longest \emph{absolute} overlap durations can only occur
with the longest word tokens, thus there is also some selection pressure
towards longer $x^{N}$ inherent in the $x^{O}$ measure. Reducing
the \isi{length} of $x^{N}$ will therefore eliminate some of the largest
$x^{O}$ tokens.} This shift is not unbounded, because \isi{speaking rate} is a bi-directional
force; high-\isi{frequency} words can also be lengthened, just not lengthened
quite as much as their lower-\isi{frequency} counterparts. Note that the
\isi{frequency} effect in this model acts on all tokens, both stored and
generated. In the latter case, we must assume that some type of motor
plan involving the concatenation of $/V_{am}/$ and $/N_{am}/$ is
associated with a \isi{resting activation} value that affects the duration
of the resulting word. 

\section{Actuation}

As in the No-Phoneme Phoneme Split model, there is no single moment
at which a sound change occurs in the Multiple-Parse Phoneme-Split
Model. Every instance of \isi{perception} involves a decision about \isi{parsing}
which is based on existing \isi{synchronic variation}. And every available
\isi{parse} is a possibility at any time, for any token; it is the probabilities
of those parses which change over time. Although the nasal vowel \isi{parse}
is assumed in the sense that it is one possible analysis for a given
token, this model, in fact, avoids many limiting assumptions about
the nature of sound change inherent to the classical view. For example,
the \emph{/V+N/} analysis is not privileged, beyond having a higher
probability of selection, given the starting distribution. Additional
analyses can be added to the set of \isi{parsing} hypotheses, if motivated
by general-purpose properties of speech \isi{perception}. It is consistently
the word level at which all forces act in this model, and at the level
of \isi{articulatory} gesture that changes are realized. In particular,
this model does not rely on the \isi{allophonic} level at which the \isi{synchronic}
rule, and the diachronic change, are assumed to occur. As a result,
the \isi{normalization}/lack of \isi{normalization} question becomes a basic element
of speech processing: the analysis that must occur when perceptual
values are transformed into \isi{production} values. 

If classification occurs at the word level, and words have \isi{articulatory}
representations something like \isi{gestural} scores, then it is not necessary
to first identify a series of abstract phonemes in order to identify
individual words. Thus, the problem of “compensating” for (the
feature of) \isi{nasalization} at the level of the \isi{phoneme} disappears. The
ambiguity remains regarding the proper \isi{articulatory} realization of
a given acoustic input, but there is no longer a unique, correct \isi{sub-lexical}
analysis. The possible analyses available to the listener, based on
their \isi{phonetic} experience, should always include, at minimum, both
the “normalized” option, as well as the “\isi{unnormalized}”
one.

In the specific simulations reported in the previous section, the
average percentage of the \isi{velum} lowering gesture that occurred simultaneously
with the \isi{tongue body} gesture varied from 30\%, for the lowest-\isi{frequency}
word, to about 50\% for the highest-\isi{frequency} word. It was also the
case that absolute gesture durations were considerably shorter
at higher frequencies. These results describe a diachronic change
under the scenario in which a single word comes to be used more, or
less, frequently over time. Under the scenario in which frequencies
are fixed, but there exists a set of words with a range of different
frequencies, these results describe a \isi{synchronic} distribution. The
model thus generates at least two testable predictions: 1) that a
difference in the degree of \isi{vowel nasalization} should be observed
across words of different frequencies (provided the relevant \isi{phonological}
context is sufficiently similar among those words), and 2) that the
highest-\isi{frequency} words should approximate the degree of \isi{vowel nasalization}
observed in languages that are described as having phonemically nasal
vowels. In other words, no exaggeration, or enhancement, of the effect
is required in this model. Lexicon-wide change is assumed to start
with change at the individual word level.

It is widely acknowledged that change (of certain kinds, at least)
happens on a word by word basis (e.g. \citealt{Phillips1984,Bybee2002,Pierrehumbert2002}),
and that some words can be ``further along'' in the change than others.
With regards to nasalized vowels in particular, \citet{malecot1960vowel}
offers evidence that the distinction between English words like \textit{cap}
and \textit{camp} is primarily that between a nasal and oral vowel
({[kæp]} vs. {[kæ̃p]}), rather than the presence
versus absence of a nasal consonant ({[kæp]} vs. {[kæ̃mp]}).
The English segmental inventory is not usually analyzed as containing
an abstract nasal vowel (although see \citealt{Sole1992} for an argument
that nasal vowels are phonologically specified). Nevertheless, individual
tokens, or individual words, or even classes of words, may have \isi{phonetic}
realizations that are indistinguishable from those generated from
an underlying nasal vowel. 
