\chapter{Free enrichment} \label{ch:free enrichment}

Everyone acknowledges the phenomenon of incomplete utterances requiring free enrichments, contents that Grice never quite foresaw. But there are significant differences in how they are perceived and, to the best of my knowledge, no one has seriously tried to present a method to derive them from first principles. Before we do so, it is helpful to set the background and context for the discussion.


\section{Representationalism and Contextualism}

Representationalism and Contextualism are two influential approaches to enrichment. The Representationalists, descendants of the logicists,\is{logicism} admit that illocutionary contents intrude into what is said\footnote{By ``what is said,'' I mean the literal meaning or content or what is generally called the \emph{truth}-conditional content, though I prefer to call it the \emph{information}-conditional content. The reason for preferring the latter is that truth conditions are not fine-grained enough to distinguish between an utterance of ``He is playing'' and ``He is playing and $2 + 2 = 4$.''} but argue that these meanings are \emph{conventionally} mandated by covert parts of the utterance in line with Grice's\ia{Grice, Paul@Grice, Paul} conventionalist conception of what is said. They allow context and inference to play a role in determining enrichments but insist that they are nevertheless triggered by ``syntactic'' elements \emph{represented} in the utterance. In other words, they see all such meanings arising from \emph{saturation}, the filling of an explicit or \emph{implicit} slot (i.e.\ a constrained variable) in a sentence. The Contextualists, descendants primarily of ordinary language philosophers, regard these contents as \emph{not} being conventionally enjoined but resulting entirely from the context.

Representationalism is a frankly rearguard action to defend a losing position. If illocutionary encroachments upon what is said are granted and the role of context and inference in their determination is allowed, then much of the battle has been lost. The conventionalist view that what is said is largely conventional \emph{has} to give ground to the contextualist view that what is said is largely contextual. Indeed, if an actual derivation from first principles is attempted, it would become obvious just how much of the former position is untenable. So bringing in conventional triggers is a feeble bulwark from this larger perspective, especially when it is grasped how rapidly these hidden variables multiply. A simple sentence with only five words like ``Every man wants to leave'' could have as many as \emph{four} ``aphonic'' or covert slots!

Sometimes the best defense is an offensive action and it is the Representationalists' criticisms of Contextualism that are important. One advantage Representationalists tout is that the slots they posit \emph{constrain} enrichments. They see context as being too unruly to be able to control what can become an indirect content. Can an utterance of ``Every Frenchman is seated'' ever convey that every Frenchman or Dutchman is seated, and can an utterance of ``Everyone loves Sally'' ever convey that everyone loves Sally and her mother?\footnote{See \citet[Chapter~7]{stanley:lc} and \citet[Introduction]{recanati:tcp} for a discussion of these examples.} Representationalists and Contextualists both share the intuition that they cannot, though the Contextualists seem unable to rule out such additions to the direct content as they appear not to have any access to the kinds of constraints associated with the slots of the Representationalists. This is the \emph{overgeneration problem} for the Contextualists: they have no mechanism to outlaw certain types of seemingly unwarranted indirect or illocutionary contents. \citet{recanati:tcp}, one of the foremost proponents of Contextualism, has even conceded that modulation is \emph{unsystematic}, that is, that no principled derivation of modulations is possible.

This has led to a kind of impasse because Representationalism is just too\linebreak baroque to be taken at face value and Contextualism appears unable to subdue problems like overgeneration. In my view, many of these difficulties stem from the fact that neither side in this debate actually knows how to derive any content, whether illocutionary or even locutionary (e.g.\ ``straightforward'' disambiguation of a word like ``bank'' or ``ran'' in an utterance as presented in \partref{part:III}).\footnote{I have already argued in \sectref{sec:Relevance Theory} that the mechanism offered by \isi{Relevance Theory} is seriously flawed.} If they did, many aspects of this situation would no longer require an appeal to questionable intuitions.

I will show how to solve, even \emph{dissolve}, the overgeneration problem. Both sides take indirect contents to be an \emph{either-or} or \emph{zero-one} matter. Either some content is possible or it is not. I will argue not only that almost any indirect content can attach to any direct content in some context or other -- \emph{contrary} to the common intuition of Representationalists and Contextualists -- but also that content as such is often more nuanced than being a simple \emph{all or nothing} issue. \citet[39--40]{grice:lc, grice:landc} himself more or less \emph{required} such indeterminacies\is{indeterminacy} in the calculation of implicatures but this subtle point has been largely forgotten. Indeed, the procedure I outline applies equally to \emph{all} indirect contents \emph{including} implicature, and once this is recognized, it becomes clear that overgeneration is a potential problem for implicature as well.\footnote{As I said in \sectref{sec:theory of conversation}, Grice's own mechanism is also seriously flawed and, in any case, it provides little protection against overgeneration because the mechanism is so vague.} But the real lesson from this observation of a uniform approach to all illocutionary contents is this: since no slots are possible for implicatures as they would be too limiting and even perhaps a little absurd, so no slots are necessary for other illocutionary contents like enrichment and modulation either! If a method can be shown to work for implicatures without slots, then the same method is likely to work for other indirect contents as well. 



\section{How to think about content}\label{page:content} \label{sec:how to think about content}

Early on, \citet{frege:sr} and \citet{russell:d} and others who followed attended to just the putative single literal content of a \emph{sentence}. The pervasive ambiguities of utterances were seldom acknowledged, let alone dealt with.\footnote{To repeat, by ``ambiguity'' I mean something broader than mere lexical and structural ambiguity, though these are the plainest kinds. I intend to include all the multiple interpretations an utterance might possibly convey.} The later \citet{wittgenstein:pi} and \citet{austin:pp} did much to broaden this picture by bringing in \emph{utterances} as the bearers of meaning, but even they succumbed to the lure of single determinate contents. Grice\ia{Grice, Paul@Grice, Paul} did allow indeterminacies\is{indeterminacy} in implicature, in fact making them one of its necessary attributes. But, influenced by the logicist\is{logicism} view of what is said, he left literal content determinate and singular. This view still persists and even the indeterminacy of implicatures has been largely ignored. So while new kinds of meaning have been identified -- such as enrichments and modulations -- the idea that the overall content is singular and determinate still holds sway.

%Recall that an infon $\sigma$ is just a structured tuple of relations, individuals, and other elements of the ontology. It is used to represent the content of an utterance and is one part of the proposition expressed, which is written $c \vDash \sigma$ where $c$ is the so-called described situation and $\vDash$ stands for the \emph{supports} relation between a situation and an infon it may contain.

In \citet{parikh:rs, parikh:le}, I argued for a very different way to think about the content of an utterance. I tried to show that meaning is almost never determinate or singular. As I say in \citet[217--218]{parikh:le}:

\begin{quote}

First, the content of an utterance is not just a single infon (or proposition) as is usually assumed, but rather a collection of infons, possibly infinite in number, relative to a described situation. Second, each of these infons is just \emph{partially} present as a member of the content through a probability attached to it. Third, not every component of the content may be explicitly intended by the speaker or explicitly inferred and represented by the addressee. And lastly, the content may be different for the speaker and the addressee -- there is no ``objective'' content. This lack of objectivity is reinforced by \ldots\ various sources of indeterminacy \ldots\ . Thus, in general, the flow of information is also a partial affair, determined by the degree of overlap between the speaker's meaning and the addressee's interpretation.

A visual image one might draw of content is as follows. Given a space of points representing the entire class of infons (i.e.\ a lattice), the content for each agent would be a subspace with each point assigned a number representing its probability. Then each agent involved would have his or her subspace and the overlap between them would determine the degree and nature of the flow of information that took place between them.

\end{quote} 

This is a very different conception of content, whether literal or not. If we reflect in a na\"{i}ve way how we experience utterances in ordinary exchanges and in literature, it should be apparent that there is some plausibility to the picture I am proposing. In this view, meaning is not a \emph{singular} or \emph{binary} affair as the Representationalists and Contextualists both assume. \emph{Multiple} contents are all \emph{partially} present in the overall meaning of an utterance. This partial presence is captured via the probabilities that the respective infons or propositions are being \emph{conveyed} by the speaker; they are \emph{not} the probabilities of the contents themselves. The points in the subspaces above can be shaded darker or lighter depending on the strength of their presence.

%From this perspective, the traditional idea of a single point that is the determinate content of an utterance seems odd and overly uptight.

%One can add to the visual image above by saying that 

Regarding the third point in the quote above about the necessity of intentions, here is a quote from \citet[349]{thomason:ami} that is indirectly related to this view:

\begin{quote}

All this shows, I think, is that `Did you mean to say' is used to query people about the consequences of what they have said, without distinguishing these clearly from what they said. More generally, `Did you mean to do' does not distinguish between intentions and foreseeable side effects, which is exactly the distinction we need to make here. To achieve some progress in these negative cases, it seems that we will need to say more about the intensionality of intentions in general, and of intentions to assert in particular, and also to find tests that extract more reliable conclusions from the evidence. The philosophical background, of course, suggests that it won't be easy to make progress on these matters.

\end{quote}

This passage reveals that Thomason was grappling with this very issue of indeterminacy \is{indeterminacy} in how to attribute meaning to an utterance in the special case pertaining to what is said and its consequences. But because he was locked into the intentional and Gricean way of seeing content, he felt that progress would be difficult. As observed in \sectref{sec:speaker meaning and word meaning}, intentions are not required to be explicitly present for all that is meant. Looking to the example of $\varphi$ below, the fact that Smith's weight refers to his weight \emph{on Earth} in most ordinary situations could be part of what was meant without it being intended by the speaker, simply because we are usually on Earth and this is just taken for granted. (And it could be understood as such by the addressee without being explicitly inferred and represented in her beliefs.) It would be quite reasonable for the speaker to assent if he was asked later if ``he meant to say it.'' In addition, it is equally plausible that such contents be only partially present in the overall content of the utterance with greater or lesser probability. 

%This expanded conception of content requires a slight shift in the definition of communication and speaker meaning as I show later.

Consider the sentence $\varphi$ discussed by \citet{cl:rmp}:

\begin{exe}
\ex Smith weighs 150 lbs. ($\varphi$)
\end{exe}

An utterance of $\varphi$ by itself does not tell us whether Smith weighs 150 lbs.

\begin{enumerate}

\item when naked in the morning
\item when dressed normally after lunch
\item after lunch in heavy outer clothing
\item after being force-fed four liters of water
\item four hours after having ingested a powerful diuretic
\item on Earth
\item on the moon
\item and his height is five and a half feet

\end{enumerate}

In the appropriate circumstances, one or more of the above completions could be legitimate probabilistic meanings of $\varphi$, even \#8. It all depends on how much is shared between speaker and addressee, what is available to both in the utterance situation, and what their goals are. Indeed, there is even an indeterminate \emph{number} of possible completions for such an utterance, not all of which might have been explicitly intended by the speaker but which neither agent can entirely eliminate because the circumstances might be sufficiently ambiguous and unclear. We can often ask after an utterance has elapsed: could the speaker also have meant this? And equally often the answer will be: possibly, though maybe with rather small probabilities. It is this more generous conception of meaning I am advocating as it appears to fit the evidence better. Whether an indirect meaning is triggered or not depends on the interlocutors' goals in part and these may not always be explicit and explicitly shared. Of course, when a context is specified more or less determinately, some of these enrichments may well become intuitively implausible and their probabilities will then plummet to zero.

The solution to the overgeneration problem then is to recognize two things. First, it is almost impossible to eliminate a possible enrichment or modulation as a \emph{potential} candidate relative to \emph{some context or other}. There could always be a context that allows a certain indirect content to become plausible. Secondly, depending on the utterance situation, a potential illocutionary candidate could become a \emph{partial} presence in the overall meaning of an utterance. A probability of zero would be attached to the majority of infons simply because there are so many infons, but zero is also not so different from a vanishingly small positive number. 

These two things, possibility and probability, correspond to two steps in the method of Equilibrium Linguistics. First, the possible enrichments have to be generated via the Semantic Constraint consisting of the sub-Constraints of Relevance and Distance \emph{after} the need for them has been triggered because the direct content or some proper part of it falls short of the shared conversational goals. Second, some of these have to be retained and others discarded in a probabilistic way via the Flow Constraint. I now proceed to show how these two steps work.

\section{The Semantic Constraint: Generating possible indirect contents}

The direct or locutionary content of $\varphi$, \emph{Smith weighs 150 lbs.}, is given by the infon $\sigma^\ell \equiv \tau_0$. This content, which I will refer to as $\tau_0$, \emph{together} with each of the completions above will give rise, respectively, to the infons $\tau_i$ with $i = 1, 2, \ldots, 8$. Thus, $\tau_{1}$, for example, will be the content \emph{Smith weighs 150 lbs.\ when naked in the morning}, $\tau_{2}$ will be the content \emph{Smith weighs 150 lbs.\ when dressed normally after lunch}, and so on.

Each of these contents will be plausible candidates in some context or other. In many ordinary situations, $\tau_0$ itself will satisfy the goals in the Setting Game and so there will not be any need for a completion. But, in other situations, $\tau_0$ might fall short of these goals and then a search for completions will be triggered. I leave it to the reader to imagine such circumstances for each completion and will simply assume that they are available to trigger a search. Thus, as we have seen in \chapref{ch:distance}, the set of possible completions for $\cal B$ for each $u$ is given by $\{ x \in \hbox{Ball}^{\cal B}_u(\tau_0, \epsilon_{d,u}) \mid  \hbox{Relevance}_u(x) > \epsilon_{R,u} \}$.

If, for example, Smith's doctor is making a statement about his weight, then completions $\tau_1$ and $\tau_2$ may have high relevance and so would perhaps $\tau_5$ and $\tau_6$ although it is possible that, say, $\tau_3$ could also be included for either the speaker or the addressee. The locutionary content $\tau_0$ would also potentially be sufficiently relevant by itself. In such a scenario, the distance to these infons from $\tau_0$ might also be sufficiently small. One can estimate the distances as was done in the example of the stranded motorist. 

Given this specification of $u$, a new facet of triggering becomes visible. As I have just said, a local search for  completions would have to be triggered by some inadequacy in $\tau_0$ with respect to the conversational goals inferred from the Setting Game in $u$. But this inadequacy may not always be clear-cut because $\tau_0$ may only \emph{partially} or hazily fulfill it. In the case of the garage, this inadequacy seemed relatively straightforward to ascertain but even there it is not completely obvious. This is why the locutionary content (or a proper part of it) often figures as a probabilistic presence in the illocutionary content. This will become clearer presently.

Returning to the Semantic Constraint, if we consider only relevance and ignore distance we may end up with a content the addressee would like to infer but for which there is little warrant because it is too remote -- as argued at the end of \sectref{sec:role of relevance}. This is also the kind of case that was mentioned earlier when I raised the question whether ``Everyone loves Sally'' could ever imply the content that everyone loves Sally and her mother. This may be very relevant for the addressee to know but it may be too distant a content given what has been uttered. It all depends on the utterance situation and it may be possible to describe a context where the distance is not so great. In any case, relevance \emph{by itself} is not sufficient.

Likewise, if we include distance but drop relevance, then we may get possible completions that do not matter to the addressee and that the addressee would not have any reason to entertain. This could occur if $\varphi$ were uttered in a casual conversation where the detailed nuances of Smith's weight are irrelevant to the addressee and would never be inferred \emph{even} postdictively if he were asked for his assent or dissent. So distance by itself is also not sufficient but it should be noted that it is often less onerous to admit an irrelevant meaning than it is to admit a remote meaning because the latter may lead to more serious errors about what the speaker was trying to convey.

Nevertheless, it may be possible to develop a complete theory based only on distance and the Flow Constraint, that is, without considering relevance. This would naturally greatly simplify the picture as we would be rid of the problems (e.g.\ the intensionality of choice) the notion of relevance faces. The argument for this is that the notion of distance accounts sufficiently for the goal of the interlocutor. That is, if the distance requirement is satisfied then the relevance requirement will automatically be satisfied. In yet other words, sufficient informational proximity \emph{implies} sufficient relevance. This is an interesting conjecture. For now, I prefer to retain both sub-Constraints since it is easier to drop something later than add it in. I will return to this alternative way of developing the account in \sectref{sec:review}.

The set of candidate completions $\{ x \in \hbox{Ball}^{\cal B}_u(\tau_0, \epsilon_{d,u}) \mid  \hbox{Relevance}_u(x) > \epsilon_{R,u} \}$ does not require $\tau_{0}$ to be a part of $x$. This allows one to include possibilities where the direct content may be rejected altogether, as happens in cases of irony -- for example, an utterance of ``He is a \emph{fine} friend'' where exactly the opposite is being conveyed. I have also simplified the notation by using $\tau_{0}$ in the distance calculation rather than any \emph{part} of $\tau_{0}$ because it can and often does happen that enrichments attach to parts of the locutionary content rather than to the whole locutionary content.\footnote{See \citet[156]{parikh:le} for more details.} Not only that, it may often \emph{not} be necessary to compute the whole locutionary content before deriving a local enrichment.

Because agents are limited, they may fail to compute the set of candidate completions exhaustively. Later, someone may inquire of a speaker if some content was part of what he meant or of an addressee if that content was part of what she inferred. If it had not been evaluated earlier, these agents would need to then test that content postdictively. For example, in most ordinary situations, the completion \#6 (i.e.\ the content $\tau_{6}$) would be felt to be both of high relevance and close proximity relative to the agents' goals but neither agent may consciously pause to compute its relevance and distance because it may not appear ``salient'': it may be taken for granted implicitly and so no triggering inadequacy would be detected. It would remain latent unless it is brought up later in the exchange or an ethnographer asks the interlocutors about it.

Not only are the assessments of relevance and distance rough and subjective, the two thresholds are also rough and subjective, and all these things vary from situation to situation too. So it is not unlikely that the speaker and different addressees come up with somewhat different possible enrichments. This is part of the indeterminacy of any utterance. What about completion \#8 or $\tau_{8}$? Can it ever, so to speak, cross these thresholds? I don't see why not. Certainly, its relevance may be high in $u$ as the addressee may be interested in Smith's height. Its distance from $\tau_{0}$ would generally also be high rather than low as required because it is unlikely to bear an inferential relation to $\tau_{0}$ in most circumstances where the goals do not permit it. So it would generally be eliminated as a possibility. But weight is roughly correlated with height within a certain range and if two doctors were discussing Smith's weight it is conceivable that his height could figure as an indirect content even if only his weight was made explicit because the distance between the two would be relatively low in such circumstances relative to their goals. 

While the inferential relations between weight and height may be partly physiological, the possible inferential relations between Sally and her mother may be social and the possible inferential relations between Frenchmen and Dutchmen may be social or historical. Even merely hypothetical inferential relations can contribute to the candidacy of an indirect content by making the relevant distances low enough. So almost any indirect content can be added to any direct content given the right circumstances and goals if the inferential relations between the infons make the distance between them small assuming the infon also has high relevance. The reason why such enrichments appear impossible in ordinary circumstances is that they are generally \emph{inferentially unrelated} to the direct content. That is, the distance lies outside the open ball in ordinary circumstances as no appropriate goal has been specified.

It now becomes clear that not only are slots \emph{not} required for triggering the search for possible enrichments (or modulations), they may in fact be too constraining if they prevent certain types of possibilities and they may be too lax in  licensing other possibilities. This is an interesting twist to the overgeneration problem because in a certain sense this observation \emph{dissolves} the problem altogether. The trouble with positing slots is that they are \emph{always} present in the sentence itself quite independent of the circumstances and, in particular, goals. This means they have no connection to what is actually going on in the communication and so will sometimes undergenerate (e.g.\ as in the examples of weight and height or Sally and her mother) and sometimes overgenerate (e.g.\ when no completion is warranted). Goals and the circumstances are appropriately connected to the communication and they, therefore, generate just the right completions for every utterance.

Here is a quote from \citet[11]{recanati:tcp}:

\begin{quote}

I agree with Stanley that certain things don't happen, that would happen if modulation were totally unconstrained. But who claimed that modulation was totally unconstrained? Work in this area precisely needs to address the issue of what is possible and what is not (step one), in order to arrive at suitable generalizations (step two), which it will then be incumbent upon pragmatic theory to derive (step three) \citep{elbourne:ab}.

\end{quote}

I believe the criterion involving Relevance and Distance does carry out all three steps. In a related footnote, Recanati says, ``One obvious constraint, implicit in my writings on the topic, is that modulation should preserve semantic types, just as adjunction preserves syntactic types.'' If such a constraint on semantic types really does turn out to be required, it would automatically be handled via the Distance sub-Constraint because two infons involving different types might naturally be too remote in \emph{most} situations. This situated way of bringing in possible constraints on semantic types is better because it allows a more contextual formulation of the generalization. Language is usually too rich for unsituated ``semantic'' diktats to work.

More conscious and deliberate processes of determining candidate completions and other illocutionary candidates are required when interpreting literature. Consider this poem by Emily \citet[18]{dickinson:p}:

%more complex utterances of the kind that occur in

\begin{quote}
%\begin{center}
%\noindent \hspace*{.8em}        % inset the poem so the page is less blocky
% % %   \begin{tabular}[b]{l}
Tell all the Truth but tell it slant --\\
Success in Circuit lies\\
Too bright for our infirm Delight\\
The Truth's superb surprise\medskip\\
% % %   \end{tabular}
%~\\
%~\\
% % %   \begin{tabular}[t]{l}         % second stanza to right and lower than first
As Lightning to the Children eased\\
With explanation kind\\
The Truth must dazzle gradually\\
Or every man be blind --
% % \end{tabular}

%--- Emily Dickinson, \emph{Poems}
%\end{center}
\end{quote}

\noindent The broad meaning of the poem appears to be fairly clear, but what about its nuances? What, for example, is to be included in ``all the Truth?'' Interpreting such utterances requires some \emph{creativity} and not only because the context for the text is incompletely specified and, indeed, specifiable. There is a range of possibilities and even coming up with them involves some imagination. This can now be explained by the cost of certain inferential paths being higher and requiring greater effort. One may have to consciously but informally judge distances as part of an extended deliberation about the poem. This is part of the reason why literary criticism is a professional activity. And it is part of the pervasive indeterminacy of language and meaning. 

Deciding which candidates are in fact part of the poem's meaning is another matter altogether. It is to this latter problem, the matter of deciding what in fact \emph{is} the content of an utterance, that I now turn. This brings in a further filtering action, this time via game theory.


\section{The Flow Constraint: Eliminating possible indirect contents} \label{sec:eliminating possible indirect contents}

For the sake of concreteness, I will assume that the Relevance and Distance criterion is triggered and generates just $\tau_{1}$, $\tau_{2}$, and $\tau_6$ as possible indirect contents for our example. The rest can be deemed to be either irrelevant or too distant or both.

The first thing to notice is that some items in the list have to be mutually exclusive whereas others can coexist. Both $\tau_{1}$ and $\tau_{2}$ cannot hold simultaneously though they can both coexist with $\tau_6$. This introduces a slight complication in how we evaluate the possibilities. Sets of mutually exclusive candidates have to be considered together in the same game whereas those that do not conflict must be treated separately.

The second thing to notice is that $\tau_6$ may never be consciously entertained by either speaker or addressee. It is, so to speak, \emph{fully} situated though it may come up later in the conversation in an explicit way. 

%For the moment, I will ignore this aspect of $\tau_6$ and treat it just like any other candidate.

Let us start with the games that arise from contemplating $\tau_1$ and $\tau_6$. Both games will involve comparing the relative merits of just $\tau_0$, the direct or locutionary content, with the possible indirect or illocutionary contents $\tau_1$ and $\tau_6$. Take a look at the illocutionary game $g^\iota_1 \equiv g^\iota_u(\varphi)$ in Figure~\ref{fig:ic1 game}.\footnote{The superscript of $g^\iota_1$ is the Greek letter ``iota,'' a mnemonic for ``illocutionary.''}

\begin{figure}[htbp] 
\input{figures/pix4varphitau1algebraic.tex} 
\caption{The game $g^\iota_1$ for illocutionary content $\tau_1$}
\label{fig:ic1 game}
\end{figure}

This is a partial information game as before but it is an \emph{illocutionary}  partial information game. $\cal A$ utters $\varphi = \Expression{Smith weighs 150 lbs.}$ in $u$, its optimal locutionary meaning $\sigma^\ell \equiv \tau_0 = \emph{Smith weighs 150 lbs.}$ (and optimal parse $t$) are determined by $\cal B$ via the locutionary game $LG_u(\varphi)$, and then they trigger various illocutionary enrichments that need to be derived via corresponding illocutionary games.

I have avoided subscripting the initial situations $s$ and $s'$ which should be $s_1$ and $s'_1$. The reader should read the symbols as carrying the appropriate intentions to convey $\tau_0$ and $\tau_1$ in this game and appropriate contents in later games that will soon appear. The payoffs are as they were in the locutionary games and obey the same inequalities. The symbols for the prior probabilities are $\varrho_1$ and $\varrho'_1$ and are distinct from the earlier symbols of \partref{part:III} which were $\rho_1$ and $\rho'_1$. They are different forms of the Greek letter ``rho.''

These prior probabilities are also conditional probabilities like those of the locutionary games of \partref{part:III}. In particular, $\varrho_1 = P(\tau_0 \cond \tau_0; u)$. Recall the meaning of the probability notation: $P(\tau_0 \cond \tau_0; u)$ indicates the probability $P({\cal A}\ \text{is conveying}\allowbreak \tau_0\ \hbox{in}\ g^\iota_1 \cond {\cal A}\ \text{is conveying}\ \tau_0\ \hbox{in}\ LG_u(\varphi); u)$  or, more simply, $P({\cal A} \text{ is conveying } \tau_0\linebreak \text{indirectly} \cond {\cal A} \text{ is conveying } \tau_0 \text{ directly}; u)$. Here, the first occurrence of $\tau_0$ is part of the illocutionary game $g^\iota_1$ whereas the second occurrence of $\tau_0$ is part of the locutionary game $LG_u(\varphi)$. If the two games had been the same, then obviously $P(\tau_0 \cond \tau_0; u)$ is just 1. But we are trying to determine whether $\tau_0$ is also an indirect content \emph{given} that it is a direct content. As I said a short while ago, a locutionary content may be hazily inadequate and so it may occur probabilistically as an indirect content as well.

%\equiv P(\tau_0 \cond \sigma^\ell; u)

%\equiv P({\cal A}\ \hbox{is conveying}\ \tau_0\ \hbox{in}\ g^\iota_1 \cond {\cal A}\ \hbox{is conveying}\ \sigma^\ell\ \hbox{in}\ LG_u(\varphi); u)$

In the same way, $\varrho'_1 = P(\tau_1 \cond \tau_0; u)$. Here, there is no clash between the symbols of different games as the indirect content is $\tau_1$ and the direct content is $\tau_0$.

We can naturally allow $t$, the optimal parse of $\varphi = \Expression{Smith weighs 150 lbs.}$, to also influence these indirect contents by simply adding it in as follows: $\varrho_1 = P(\tau_0 \cond \tau_0, t; u)$ and $\varrho'_1 = P(\tau_1 \cond \tau_0, t; u)$. Whether $t$ actually influences the indirect content is an empirical question; my purpose here is just to show that both situations can be easily accommodated in the framework.

This also raises the much more interesting question whether indirect meanings influence direct meanings and parses. If this is the case, then again it is very easy to capture this simply by adding in the relevant indirect meaning $\tau_0$ or $\tau_1$ as a \emph{conditioning} variable that would play a role in determining locutionary meanings and parses. And simultaneous two-way influence can be readily entertained by having variables such as $x'_1$ for the indirect meanings $\tau_0$ and $\tau_1$ and variables such as $x_i$ and $y_j$ for direct meanings and parses. Then, everything would be one grand system of equations encompassing both locutionary and illocutionary contents. However, there may be no empirical warrant for this in ordinary face-to-face communication and one should be cautious about advocating such a comprehensive system. When reading is involved, on the other hand, especially with literature where interpretive efforts are more likely to be extended, such a comprehensive system is more likely to be present than not, as a reader may consciously reject a direct meaning and parse because their indirect meanings prove unsatisfactory relative to the indirect meanings of alternative direct meanings and parses. In such cases, the direct meaning and parse are selected on the basis of their more satisfying indirect meanings and so depend on them. In general, the matter should be decided purely on empirical grounds and my purpose here is just to show that both kinds of possibilities can be effortlessly represented in the framework of Equilibrium Linguistics. An entirely new effect that can be conceived is the influence of indirect meanings on optimal parses and vice versa.

One such example of a seemingly common type that can occur even in face-to-face communication is as follows. Suppose $\cal A$ and $\cal B$ are talking about Russia and $\cal B$ asks: ``Is Russian literature any good?'' Then $\cal A$ may respond: ``Well, Dostoyevsky was Russian.'' Then ``Dostoyevsky''\ia{Dostoyevsky, Fyodor@Dostoyevsky, Fyodor} could refer to either the novelist or other Russians with the same last name and it seems it is the implicature that Russian literature is great that enables the addressee to choose the novelist. In other words, the indirect meaning helps to determine the direct meaning and vice versa.

One difficulty such a comprehensive system faces is in explaining why suboptimal direct meanings and parses do not seem to generate their own illocutionary possibilities which would then feed back into bolstering them, however feebly. For example, $\varphi = \Expression{Smith weighs 150 lbs.}$ has the alternative suboptimal (in $u$) locutionary meaning $\tau'_0$ that Smith determines the weight of \pounds 150 (in coins). This suboptimal meaning would require its own enrichments (e.g.\ \emph{in coins}) but we are not generally aware of them. We are not generally aware of the locutionary meaning $\tau'_0$ either but, as mentioned in \sectref{sec:psycholinguistics}, we seem to unconsciously activate alternative \emph{lexical} meanings. Most likely, the processing does not go beyond this to a full-scale sentential referential meaning such as $\tau'_0$ and so the warrant for alternative indirect meanings triggered by $\tau'_0$ may be weak in such face-to-face interactions \emph{unless} the indirect meanings result from a proper part of $\tau'_0$. In any case, in more literary situations, such interdependent effects are likely to be more common.

From a computational standpoint, too, it may be interesting to consider such comprehensive influences as they provide some more data for eliminating unwanted possibilities though they also substantially complicate the calculations. To keep things simple in this book, I will assume just a one-way influence (without the intervention of the optimal parse $t$) and take $\varrho_1 = P(\tau_0 \cond \tau_0; u)$ and $\varrho'_1 = P(\tau_1 \cond \tau_0; u)$.\footnote{Such two-way influences were explicitly considered in my previous book in Chapter~4.}

%In theory, it is possible to consider a slightly different form for the game $g^\iota_1$ shown in Figure~\ref{fig:ic1 game} where no alternative locutions are shown for $\cal A$, just the single option $\varphi$. If we wish, we could identify a game of the kind shown in Figure~\ref{fig:old semantic lexical game g1} in \sectref{sec:old partial information game} where alternative actions are available to the speaker as well. To me, it seems that such actions are not contemplated by speakers (except in the Generation Game $GG_u(\varphi)$ shown in Figure~\ref{fig:GG} in \sectref{sec:generation game}) but I mention this just in case the experimental evidence does reveal that they are. These ``old'' partial information games complicate the solution process but only a little. For our purposes, I will avoid these distractions and stick to the ``new'' form of partial information game $g^\iota_1$.

%\begin{figure}[htbp] 
%\input{figures/pix4varphisigma1algebraic.tex} 
%\caption{The game $g^\iota_1$ for illocutionary content $\sigma_1$}
%\label{fig:ic1 old game}
%\end{figure}

%To those unfamiliar with the setup, $s$ is an initial situation in which the speaker has an intention to convey $\sigma_0$ and may utter either $\varphi$, our example sentence, or an explicit and unambiguous alternative such as $\varphi'$, which could be $\varphi$ itself but understood as conveying just the direct content without any enrichments. It could also be something like ``Smith weighs 150 lbs., period,'' although this option introduces some extraneous complexities about how the content of ``period'' should be rendered. 
%
%Alternatively, one can imagine that the branch labeled $\varphi'$ (and its follower, the branch labeled $\sigma_0$ with payoffs $b_{\cal A}, b_{\cal B}$) are just omitted from the game tree. Either way would be fine.


Even to the newcomer to game theory, $g^\iota_1$ should now be easy to solve. All that is required is a simple Nash equilibrium that can be computed by evaluating the inequalities between corresponding expected payoffs:

\noindent either \[ \varrho_1a_{\cal A} + \varrho'_1c'_{\cal A} < \varrho_1c_{\cal A} + \varrho'_1a'_{\cal A} \]
\noindent and \[ \varrho_1a_{\cal B} + \varrho'_1c'_{\cal B} < \varrho_1c_{\cal B} + \varrho'_1a'_{\cal B} \]

\noindent or \[ \varrho_1c_{\cal A} + \varrho'_1a'_{\cal A} > \varrho_1a_{\cal A} + \varrho'_1c'_{\cal A} \]
\noindent and \[ \varrho_1c_{\cal B} + \varrho'_1a'_{\cal B} > \varrho_1a_{\cal B} + \varrho'_1c'_{\cal B} \]

\noindent which simplifies to: \[ \frac{\varrho_1}{\varrho'_1} \stackrel{<}{>} \frac{a'_{\cal A} - c'_{\cal A}}{a_{\cal A} - c_{\cal A}} \]

\noindent and \[ \frac{\varrho_1}{\varrho'_1} \stackrel{<}{>} \frac{a'_{\cal B} - c'_{\cal B}}{a_{\cal B} - c_{\cal B}} \]

\noindent As assumed in \sectref{sec:solving locutionary global games}, if the payoffs are ``symmetric'' then each of the two numerators on the right-hand side equals the corresponding denominator on the right-hand side, and this leaves us simply with: \[ \varrho_1 < \varrho'_1\ \  \hbox{or}\ \ \varrho_1 > \varrho'_1 \]

\noindent Often, there will be no situational evidence to warrant either strict inequality and we will simply have: \[ \varrho_1 = \varrho'_1 \]

\noindent as both possibilities will be equally likely. When this happens, an infinite number of mixed strategy equilibria with some probability weight $\pi_0$ on $\tau_0$ and therefore $\pi_1 = 1 - \pi_0$ on $\tau_1$ become available. It is possible for $\pi_0$ to be 0 or 1 in which event the corresponding pure strategy, either $\tau_1$ or $\tau_0$, would be selected as the equilibrium.\footnote{I have omitted $\varphi$ from these equilibria as it is always present.} In actual practice, as I pointed out in \sectref{sec:solving locutionary global games}, all $\cal A$ and $\cal B$ have to do is compare certain probabilities, which is what the computation of Nash equilibria under the assumptions about symmetric payoffs amounts to, and this seems within the grasp of partially rational agents. Moreover, if we do not assume a two-way influence between locutionary and illocutionary meanings then the probability comparisons are much simpler. How do the agents pick the weights $\pi_0$ and $\pi_1$? Again, in practice, these are generally rough estimates like \emph{high}, \emph{medium}, and \emph{low}, which can be thought of as fuzzy intervals (but see the discussion below as well). 

%I discuss the question of possible ways for the agents to determine the probabilistic weights below.

So if $\varrho_1 < \varrho'_1$ then $\tau_1$ is the solution with probability $\pi_1 = 1$ and if $\varrho_1 > \varrho'_1$ then $\tau_0$ is the solution with probability $\pi_0 = 1$. In cases where $\varrho_1 = \varrho'_1$ and some mixed strategy solution is expected, the solution to $g^\iota_1$ might be $(\tau_1, \pi_1)$ with the understanding that this implies the complementary solution $(\tau_0, \pi_0)$. For all situations then, we can write the general form of the content roughly as collections of sets like $\{(\tau_i, \pi_i) \cond i \in I\}$ where $\tau_i$ are the range of indirect contents, $\pi_i$ their probabilistic weights as derived from the relevant games $g^\iota_i$, and $I$ an index set. For example, the game in Figure~\ref{fig:ic6 game} might have the solution $(\tau_6, \pi_6)$ where $\tau_6 = \emph{Smith weighs 150 lbs. on Earth}$. Both of these indirect solutions would be gathered together and the indirect content expressed as $\{\{(\tau_1, \pi_1)\}, \{(\tau_6, \pi_6)\}\}$. 

\begin{figure}[h] 
\input{figures/pix4varphitau6algebraic.tex} 
\caption{The game $g^\iota_6$ for illocutionary content $\tau_6$}
\label{fig:ic6 game}
\end{figure}


Intuitively, this tells us that $\tau_1$ (i.e.\ \emph{Smith weighs 150 lbs.\ when naked in the morning}) is a meaning of the utterance of $\varphi$ in some situation $u$ with a probability of $\pi_1$ (which would be some number like 0 or 0.3 or 0.7 or some interval) and that $\tau_6$ (i.e.\ \emph{Smith weighs 150 lbs.\ on Earth}) is also a meaning of the same utterance with a probability of $\pi_6$ (also a number or interval, presumably higher, like 0.9 or even 1 depending on $u$). In other words, there will be a range of indirect contents, each with some probability weight. Obviously, as I said earlier, these weights are not the probabilities of the infons themselves but the probabilities that the respective contents are being conveyed by the speaker.

When mutually exclusive indirect contents are considered, they have to be evaluated simultaneously in the same game. So the game for $\tau_1$ and $\tau_2$ would not be $g^\iota_1$ and a corresponding $g^\iota_2$ similar to it, but would be $g^\iota_{12}$ as shown in Figure~\ref{fig:three node game} where I have deliberately left out the payoffs. In this case, the mixed strategy solution would be written as $\{(\tau_0, \pi_0), (\tau_1, \pi_1), (\tau_2, \pi_2)\}$ with the natural proviso that $\pi_0 + \pi_1 + \pi_2 = 1$.

 \begin{figure}[htb]
 \begin{center} %FIGURE 7
%\begin{picture}(216,250)(0,-18)
 \begin{picture}(216,235)(60, -5)
 %--------------------------------
 % NODES
% \put(0,27){\circle*{3}}
% \put(0,108){\circle*{3}}
% \put(0,189){\circle*{3}}
% \put(54,27){\circle*{3}}
% \put(54,108){\circle*{3}}
% \put(54,189){\circle*{3}}
 \put(108,27){\circle*{6}}  %here
 \put(108,108){\circle*{6}} %
 \put(108,189){\circle*{6}} %
 \put(162,27){\circle*{3}}  
 \put(162,108){\circle*{3}} 
 \put(162,189){\circle*{3}} 
 \put(216,0){\circle*{3}}
 \put(216,27){\circle*{3}}
 \put(216,54){\circle*{3}}
 \put(216,81){\circle*{3}}
 \put(216,108){\circle*{3}}
 \put(216,135){\circle*{3}}
 \put(216,162){\circle*{3}}
 \put(216,189){\circle*{3}}
 \put(216,216){\circle*{3}}
 %--------------------------------
 % LINES and ARROWS
% \put(0,27){\line(1,0){54}}
% \put(0,108){\line(1,0){54}}
% \put(0,189){\line(1,0){54}}
% \put(54,27){\line(1,0){54}}
% \put(54,108){\line(1,0){54}}
% \put(54,189){\line(1,0){54}}
 \put(108,27){\line(1,0){54}}
 \put(108,108){\line(1,0){54}}
 \put(108,189){\line(1,0){54}}
 \put(162,27){\line(1,0){54}}
 \put(162,108){\line(1,0){54}}
 \put(162,189){\line(1,0){54}}
 \put(162,27){\line(2,-1){54}}
 \put(162,27){\line(2,1){54}}
 \put(162,108){\line(2,-1){54}}
 \put(162,108){\line(2,1){54}}
 \put(162,189){\line(2,-1){54}}
 \put(162,189){\line(2,1){54}}
 %-------------------------------
 % OVAL
 \put(162,108){\oval(15,192)}
 %-------------------------------
 % LABELS
% \put(54,36){\makebox(0,0){$e''$}}
% \put(54,117){\makebox(0,0){$e'$}}
% \put(54,198){\makebox(0,0){$e$}}
 \put(108,36){\makebox(0,0){$s''$}}
 \put(108,117){\makebox(0,0){$s'$}}
 \put(108,198){\makebox(0,0){$s$}}

\put(108,18){\makebox(0,0){$\varrho''$}}
 \put(108,99){\makebox(0,0){$\varrho'$}}
 \put(108,180){\makebox(0,0){$\varrho$}}

% \put(162,36){\makebox(0,0){$t''$}}
% \put(162,117){\makebox(0,0){$t'$}}
% \put(162,198){\makebox(0,0){$t$}}
% \put(27,33){\makebox(0,0){$\sigma_2$}}
% \put(27,114){\makebox(0,0){${\sigma_1}$}}
% \put(27,195){\makebox(0,0){$\sigma_0$}}
% \put(81,33){\makebox(0,0){$\varphi'''$}}
% \put(81,114){\makebox(0,0){$\varphi''$}}
% \put(81,195){\makebox(0,0){$\varphi'$}}
 \put(135,33){\makebox(0,0){$\varphi$}}
 \put(135,114){\makebox(0,0){$\varphi$}}
 \put(135,195){\makebox(0,0){$\varphi$}}
 \put(191,33){\makebox(0,0){$\tau_1$}}
 \put(191,114){\makebox(0,0){$\tau_1$}}
 \put(191,195){\makebox(0,0){$\tau_1$}}
 \put(189,20){\makebox(0,0){$\tau_2$}}
 \put(189,101){\makebox(0,0){$\tau_2$}}
 \put(189,182){\makebox(0,0){$\tau_2$}}
 \put(189,47){\makebox(0,0){$\tau_0$}}
 \put(189,128){\makebox(0,0){$\tau_0$}}
 \put(189,209){\makebox(0,0){$\tau_0$}}
 \put(233,0){\makebox(0,0){$$}}
 \put(238,27){\makebox(0,0){$$}}
 \put(238,54){\makebox(0,0){$$}}
 \put(238,81){\makebox(0,0){$$}}
 \put(238,108){\makebox(0,0){$$}}
 \put(238,135){\makebox(0,0){$$}}
 \put(238,162){\makebox(0,0){$$}}
 \put(233,189){\makebox(0,0){$$}}
 \put(238,216){\makebox(0,0){$$}}
 \put(-17,27){\makebox(0,0){$$}}
 \put(-17,108){\makebox(0,0){$$}}
 \put(-22,189){\makebox(0,0){$$}}
 %-------------------------------
 \end{picture}
 \caption{The game $g^\iota_{12}$ with two illocutionary contents $\tau_1$ and $\tau_2$} \label{fig:three node game}
 \end{center}
 \end{figure}


If we now consider the overall content of the utterance, assuming that only items \#1, 2, and 6 or $\tau_1$, $\tau_2$, and $\tau_6$ get through the Relevance plus Distance filter, it would have to include $\tau_0$ with probability 1 as the direct content,\footnote{The probability that some direct content is being conveyed is not always 1. With a pun, it might be $0.5$.} it would then include $\tau_0$, $\tau_1$, and $\tau_2$ as one set of mutually exclusive indirect contents with probabilities $\pi_0$, $\pi_1$, and $\pi_2$, respectively, and, last, it would include $\tau_0$ and $\tau_6$ as indirect contents with probabilities $\pi'_0$ and $\pi_6$. This content would be expressed as follows: 
\begin{multline*}
\{\ \{(\tau_0, 1)\},\ \{(\tau_0, \pi_0), (\tau_1, \pi_1), (\tau_2, \pi_2) \cond \pi_0 + \pi_1 + \pi_2 = 1\},\\ \{(\tau_0, \pi'_0), (\tau_6, \pi_6) \cond \pi'_0 + \pi_6 = 1\}\ \}
\end{multline*}

\noindent If we make the constraints on the probabilities implicit, this simplifies to: \[\{\ \{(\tau_0, 1)\},\ \{(\tau_0, \pi_0), (\tau_1, \pi_1), (\tau_2, \pi_2)\},\ \{(\tau_0, \pi'_0), (\tau_6, \pi_6)\}\ \}\]

\noindent This is \emph{exactly} the form of the content I referred to earlier in \sectref{sec:how to think about content} where I said it was a collection of infons each with a probability. This can be visualized on the infon lattice as suggested earlier. The entire proposition expressed would be: \[c \vDash \{\ \{(\tau_0, 1)\},\ \{(\tau_0, \pi_0), (\tau_1, \pi_1), (\tau_2, \pi_2)\},\ \{(\tau_0, \pi'_0), (\tau_6, \pi_6)\}\ \}\]

\noindent where $c$ is some appropriate described situation. For this example, this would be the \emph{literal} content of the utterance. It includes both the locutionary content $\sigma^\ell \equiv \tau_0$ with probability 1 as well as various indirect contents that have crossed the Relevance plus Distance barrier. In other examples, there may be modulations and other indirect contents to consider as well as we will see in the next two chapters.

Note that $\pi_0$ is in general different from $\pi'_0$ and both are in general different from 1, and yet these three numbers all attach to the same content $\tau_0$. This requires some interpretation. The direct content $\tau_0$ (i.e.\ \emph{Smith weighs 150 lbs.}) is fully part of the overall content with probability one in this example. Its presence in the other indirect contents is \emph{relative} to the other indirect components (i.e.\ either $\tau_1$ and $\tau_2$, or $\tau_6$). It provides a constraint on the other indirect infons by limiting their participation in each indirect content.

%but its own presence is determined by the process of deriving the locutionary content of $\varphi$.

%, which is based on its constituent words and phrases as discussed in detail in \partref{part:III}.

Also, the speaker and addressee are unlikely to share the same content not only because they may have generated different possibilities but also because the game-theoretic computations may yield different solutions. In other words, the single content expressed above would split into two separate contents, one for the speaker and the other for the addressee, with possibly different infons and possibly different probabilities. This again is just a related aspect of indeterminacy. These literal meanings would look as follows:
\[c^{\cal A} \vDash \{\ \{(\tau_0, 1)\},\ \{(\tau_0, \pi^{\cal A}_0), (\tau_1, \pi^{\cal A}_1), (\tau_2, \pi^{\cal A}_2)\},\ \{(\tau_0, \pi^{{\cal A}'}_0), (\tau_6, \pi^{\cal A}_6)\}\ \}\]

\noindent and
\[c^{\cal B} \vDash \{\ \{(\tau_0, 1)\},\ \{(\tau_0, \pi^{\cal B}_0), (\tau_1, \pi^{\cal B}_1), (\tau_2, \pi^{\cal B}_2)\},\ \{(\tau_0, \pi^{{\cal B}'}_0), (\tau_6, \pi^{\cal B}_6)\}\ \}\]

We should remind ourselves that the illocutionary games $g^\iota_i$ I have been discussing belong to the \emph{objective} illocutionary global game $IG_u(\varphi)$ which is a part of the objective global game $G_u(\varphi) = LG_u(\varphi) \cup IG_u(\varphi)$, as I have said earlier. There are naturally the corresponding \emph{subjective} global illocutionary games $IG^{\cal A}_u(\varphi)$, $IG^{\cal B}_u(\varphi)$ which are in turn parts of the corresponding subjective global games $G^{\cal A}_u(\varphi) = LG^{\cal A}_u(\varphi) \cup IG^{\cal A}_u(\varphi)$ and $G^{\cal B}_u(\varphi) = LG^{\cal B}_u(\varphi) \cup IG^{\cal B}_u(\varphi)$. To stay within the bounds of \emph{communication} as delineated by Definition~\ref{def:communicates} in \sectref{sec:speaker meaning and word meaning}, I have simply assumed that $u$ is such that $G_u(\varphi) = G^{\cal A}_u(\varphi) = G^{\cal B}_u(\varphi)$ and $IG_u(\varphi) = IG^{\cal A}_u(\varphi) = IG^{\cal B}_u(\varphi)$ and, further, that 
they all become (nonconscious) common knowledge between $\cal A$ and $\cal B$. But, especially with illocutionary games, it is all too common that there is miscommunication or a weaker flow of information than strict communication and this is then often explained by a divergence among these three games or by the lack of common knowledge. 

Indeed, the solutions even to identical subjective games that are common\linebreak knowledge can diverge because different probabilistic weights $\pi^{\cal A}$, $\pi^{\cal B}$ may attach to these contents. Since some of the prior probabilities in such games are often equal, an infinite number of mixed strategy equilibria result, and in these circumstances $\cal A$ and $\cal B$ may well arrive at different weights for various indirect contents.

There appear to be two possible sources of such weights. Recall that $\{ x \in \hbox{Ball}_u(\tau_0, \epsilon_{d,u}) \mid  \hbox{Relevance}_u(x) > \epsilon_{R,u} \}$ is the set of candidates that contains infons that are both sufficiently close and sufficiently relevant. However, the distance and relevance of each candidate will, in general, be different. Those candidates with greater proximity and greater relevance ought to be more probable than those candidates with lower proximity and lower relevance as they are the contents more likely to be conveyed by the speaker. Thus, the probabilistic weights $\pi^{\cal A}$, $\pi^{\cal B}$ come from the \emph{relative} proximity and relevance of different indirect contents in the solution. If there had been just one criterion, either just Distance or just Relevance, the proportion of the probabilistic weights would just be the same as the proportion of either distance or relevance. Since there are two criteria, some suitable function of the two is required to work out the ratio of the probabilistic weights. 

%Many different possibilities exist and I will not pick out one.

%For example, I said in \chapref{ch:distance} that the implicature $\tau$ (i.e.\ \emph{$\cal A$ thinks the garage might be open}) is both closer and more relevant than the implicature $\tau'$ (i.e.\ \emph{the gas available there is inexpensive}). This implies that $\tau$ is \emph{proportionately} more probable than $\tau'$ in the solution to the corresponding Flow Constraint for the utterance of $\psi$ in $u$.

There will also be situations when one prior probability outweighs the others and then the corresponding illocutionary content could be the pure strategy equilibrium for both agents with probability 1. This outcome is not unusual but since there will always be a range of indirect contents, it is likely that at least some of them will enter the literal content probabilistically. For example, it may be that, in the above two contents for $\cal A$ and $\cal B$, $\pi^{\cal A}_6 = \pi^{\cal B}_6 = 1$ but that the other probability weights lie strictly between 0 and 1.

There may be some interest in actually \emph{measuring} just how much $\cal B$'s inferred meaning \emph{diverges} from $\cal A$'s intended meaning.\footnote{As noted earlier in \partref{part:III}, $\cal A$ knows what he wants to convey with certainty but he nevertheless has to derive it based on the Generation Game which contains his subjective representation of the relevant games of partial information. That is, he has to figure out what $\cal B$ would infer and it is this meaning that is given by $\{\ \{(\tau_0, 1)\},\allowbreak \{(\tau_0, \pi^{\cal A}_0), (\tau_1, \pi^{\cal A}_1), (\tau_2, \pi^{\cal A}_2)\},\allowbreak \{(\tau_0, \pi^{{\cal A}'}_0), (\tau_6, \pi^{\cal A}_6)\}\ \}$.} For this, one way is to enlist the idea of \emph{relative entropy} or the so-called Kullback-Leibler divergence from information theory.\footnote{See \citet[72]{ms:fsnlp}.} Since each content is a collection of probability distributions $\pi^{{\cal A}_j}$ and $\pi^{{\cal B}_j}$, $j = 1, 2, \ldots\ $, over the same corresponding spaces of sets of infons,\footnote{I am taking $\pi^{{\cal A}_j}$ and $\pi^{{\cal B}_j}$ to be just one pair of distributions within the content. As we have just seen, there will in general be \emph{multiple} such pairs of distributions within the overall literal meaning. That is, the superscript $j$ is different from the subscripts on the probabilities $\pi$, the latter representing actual probabilities, not whole distributions.} we can compute the relative entropy for corresponding pairs of distributions which is defined as: \[ D(p \parallel q) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)} \] 

\noindent where $p$, $q$ are the two probability mass functions being compared, the logarithm is with respect to base $2$, and $X$ is the relevant set of infons. Extremal cases involving zeros are defined thus: $0 \log 0/q = 0$ for all $q$ and otherwise $p \log p/0 = \infty$. 

$D(p \parallel q)$ measures the expected number of extra bits required to code samples from $p$ when using a code based on $q$, rather than using a code based on $p$. Generally, $p$ represents the ``true'' distribution of data, observations, or a precisely calculated theoretical distribution. The measure $q$ typically represents a theory, model, description, or an approximation of $p$.\footnote{This measure of proximity between two distributions is \emph{not} a metric in the usual sense. There are other ways of measuring distance (e.g.\ Bhattacharyya distance which is given by $D_B(p, q) = - \ln(\sum_{x \in X}\sqrt{p(x)q(x)})$) which involve real metrics.}

For each pair of corresponding distributions $\pi^{{\cal A}_j}$, $\pi^{{\cal B}_j}$ with $\cal A$'s distribution substituted for $p$ and $\cal B$'s for $q$, this will yield a number. Since a content involves \emph{multiple} pairs, we can take the \emph{sum} of these numbers to find the \emph{total relative entropy} of the two contents. To ensure that we have the same corresponding pairs of distributions in $\cal A$'s and $\cal B$'s contents, we may occasionally need to introduce a distribution in either content where certain infons get a zero probability. This may pose a small problem of interpretation because when $p(x) \not = 0$ and $q(x) = 0$ the divergence would become infinite. To avoid this, we could apply some kind of smoothing operation (e.g.\ introducing a small positive number in place of the zero). 

As there is no ``true'' or ``objective'' meaning, this formula yields simply the total divergence (i.e.\ the sum of the expected number of extra bits required with respect to each pair of corresponding distributions  $\pi^{{\cal A}_j}$, $\pi^{{\cal B}_j}$) of $\cal B$'s derived meaning from $\cal A$'s intended meaning.\footnote{I have deliberately ignored the possible differences between $c^{\cal A}$ and $c^{\cal B}$.} The obvious use of this measure is to see how far apart the two are, which would give an idea of the extent to which the  communication has been a success. An identity of speaker meaning and addressee interpretation is seldom achieved and represents just an ideal case.\footnote{See \citet{parikh:ul,parikh:rs,parikh:le}.} Yet, this assumption has been the norm for much of semantics.

If there are two or more addressees and one speaker in an utterance situation, we can now compare whose meanings are closer to each other and whose further apart. Another application is to extend this formula to a further sum for entire discourses or dialogues. A third is to situations like Chinese whispers where a relay of utterances that attempts to preserve content is involved.

To recap what we have achieved: I have described the process of deriving indirect contents such as free enrichments from first principles as involving three steps: first, the need for indirect contents is triggered by the agents' goals not being met by the locutionary content, then finding candidate possibilities through local search via relevance and distance, and then solving certain games of partial information to either eliminate or include these candidates in a probabilistic way. The same kind of reasoning would enable us to derive the literal meaning of the sentence $\varphi = \Expression{Bill ran}$ from \partref{part:III}, enriching its locutionary content \emph{Bill Smith stood (for election)} to \emph{Bill Smith ran in the local election}. The steps comprise four illocutionary Constraints: the same Phonetic and Syntactic Constraint as before obviously, a new Semantic Constraint involving the sub-Constraints of Relevance and Distance which generates the possible indirect contents, and a new Flow Constraint involving illocutionary games of partial information that eliminates the suboptimal ones.

Illocutionary meanings such as $\tau_6$ (i.e.\ \emph{Smith weighs 150 lbs. on Earth}) may never be consciously entertained by either speaker or addressee. In this case, the game $g^\iota_6$ would not be played by either agent and so its solution would not enter into either agent's conscious content. It is then up to the theorist or ethnographer to impute this fully situated meaning to them. There is no mystery about this: if they were asked if they counted it as part of their understanding, they would invariably give their assent in most situations.

I have considered whether illocutionary meanings could affect locutionary meanings and parses, a possibility Equilibrium Linguistics effortlessly allows. I have chosen to err on the side of caution in this book for oral utterances by assuming that locutionary meanings influence illocutionary ones but not vice versa.

The picture of the meaning of an utterance as spread out over the entire space of infons with each point in the infon lattice assigned a probabilistic weight based on the likelihood of its being conveyed by the speaker appears very attractive as it seems to fit our empirical uncertainty about whether some (direct or indirect) content is in fact part of the meaning or not.

The Nash and Pareto-Nash equilibrium concepts more readily lend themselves to such probabilistic infons than does the Risk Dominant Nash equilibrium concept. If this probabilistic picture of content that I have informally visualized is found to be empirically accurate, then it suggests that the Nash and Pareto-Nash ideas are perhaps the more appropriate ones to consider.

In the example of implicature about the garage in \chapref{ch:distance}, the reader should have no trouble seeing how both the implicatures $\tau$  and $\tau'$ might be derived in corresponding illocutionary games, one with $\sigma^{\ell}$ and $\tau$ as $\cal B$'s choices and the other with $\sigma^{\ell}$ and $\tau'$ as $\cal B$'s choices, the solution to the first assigning a relatively high probability to $\tau$ and the solution to the second assigning a lower probability to $\tau'$. 

%On the other hand, if the driver looking for directions had been driving a Ferrari, the price of gas would have been less relevant for him and he may have eliminated this possibility at the outset.

Processing Relevance and Distance as well as the Flow Constraint may seem like a lot psycholinguistically even though the brain is very fast. This is why human agents generally derive and become aware of just a few indirect meanings, especially in oral communication. But it is clear that implicatures such as $\tau'$ can be derived if we spend more time reasoning. This is especially true when more complex forms of communication are involved as happens with literature. The inferences take more time and we are more conscious of them. This is an added reason why semantics and psycholinguistics should not be conflated.

%The addressee may well wonder on his way to the garage if the speaker had also meant $\tau'$.

%\footnote{Implicature is sometimes triggered via a possible infringement or flouting of a maxim. Such a possible or actual violation \emph{can} matter in some cases but not all. Even a possible violation does not seem to be required in this example. However, \citegen[p. 32]{grice:landc} own analysis is via a possible infringement of the maxim of relevance. In my view, a very low or even zero probability on the implicature $\tau'$ cannot be ruled out. When the speaker may not be expected to know $\tau'$, it may even be too remote and therefore not even a possibility. \label{foot: implicature}}

I have also tackled the problem of overgeneration in two stages both of which involve indeterminacies. Based on Relevance and Distance and the Flow Constraint, I argued that almost any indirect content can attach to a direct content in some (possibly outlandish) context or other. 

%This intuition was given some flesh with a criterion that combined two independent notions, relevance as the value of information and situated informational distance. This principle allowed some infons as possible candidates for the meaning of the utterance. Both Relevance and Distance provide subjective and rough guidelines and so are apt to be indeterminate and variable. In the next stage, I used games of partial information to develop the idea of probabilistic contents as a way to think about content generally. Such probabilities are a natural interpretation of one type of indeterminacy that routinely afflicts the interpretation of especially indirect or illocutionary contents.

This single argument ought to kill two birds, the overgeneration problem\linebreak raised by the Representationalists and their charge of unsystematicity that Recanati\ia{Recanati, Franois@Recanati, Franois} has acknowledged. I have tried to show by actually deriving the enriched content of a sample utterance from first principles that both can be defeated and the broad position of Contextualism defended if one adopts the ideas of Equilibrium Linguistics. This requires giving up many cherished Gricean\ia{Grice, Paul@Grice, Paul} tenets but these are suspect anyway. And my framework does a lot more: it offers more precise and robust ways of computing not just direct contents but also indirect contents, including not just enrichments but also modulations and implicatures, all using just a few uniform and more or less self-evident principles of ontology and partial rationality.

This completes my discussion of free enrichment. I now briefly consider implicature before moving on to modulation.

