\chapter{Usage-based approaches to grammar and variation}\label{UBL}

The last two decades have witnessed a remarkable advance in empiricist approaches to language, whose hallmark is a commitment to psychological realism in explaining and modelling language and human linguistic behaviour. Among these approaches, usage-based linguistics is regarded as one of the most fruitful and rapidly expanding research programmes. In a usage-based view, linguistic representations emerge and change because cognitive processes, commonly referred to as “domain-general psychological mechanisms”, operate in language use. These general and basic capacities of the human brain such as memory, categorisation, chunking, analogy, cross-modal associations, to mention only the most relevant ones, affect linguistic representations and patterns of language structure observed in human linguistic behaviour. For my account of Russian-German code-mixing, I adopt a usage-based perspective on language and language variation. This chapter presents the main assumptions underlying usage-based theories and thus lays the theoretical foundation for the studies reported in Chapters \ref{NP}--\ref{PL}. As it is beyond the scope of this chapter to give a detailed review of all the premises, findings and insights gleaned from recent usage-based work in various branches of linguistics, I restrict myself to introducing those tenets and concepts  which are basic to the usage-based approach to code-mixing as being developed in this thesis. 

To set the stage, I start by giving a brief description of linguistic representations posited by usage-based exemplar models. In light of the focus of this book on multimorphemic words and multiword sequences (henceforth, multimorphemic elements), the following section will scrutinise aspects of their acquisition, comprehension and production. The final section introduces usage-based approaches to language variation, which is viewed as emergent from competition between functional linguistic units and communication strategies in interaction. Additionally, to supply the reader with an illustration of a motive for competition, I will elaborate on the cognitive process of priming and discuss it in the light of usage-based models.

\section{Rich memory for language: Exemplars, networks and constructions}
\label{sec:exemplars}

Usage-based models posit that the language user's memories of specific language experiences are stored in the brain as exemplars. A specific language experience leads to the activation of numerous exemplars along multiple dimensions. This section will show how exemplars are organised and how new linguistic experience updates networks of exemplars. Further, it will elaborate on the emergence of abstract structure, most notably of schematic constructions, through generalisations over exemplars as an emergent consequence of network organisation.

The usage-based position builds on the fact that the human brain has a very large mental storage capacity \citep[126]{sherwood-fundamentals-2012}. The brain keeps track of a human being's experience through storing detailed information about her specific experiences as memories. With regard to language, these memories comprise ``phonetic detail, including redundant and variable features, the lexical items and constructions used, the meaning, inferences made from this meaning and from the context, and properties of the social, physical and linguistic context'' (\citealt[14]{bybee-book-2010}; \citealt[cf.][]{langacker-foundations-1987,langacker00}). In other words, long-term memory representations abound in fine-grained information about experience with language.

Usage-based theory holds that linguistic memories are organised as exemplars, which are representations formed of individual tokens of linguistic experience \citep{johnson97, bybee-phonology-2001,bybee-book-2010,pierrehumbert-2001}. Exemplars and their clusters, or clouds, emerge in the process of categorisation, which is a cognitive mechanism whereby a new token of experience is compared against an established exemplar with the purpose of determining whether it belongs to the same category. This process is carried out as follows: If the organism considers a new token of experience and an existing exemplar the same, the new token strengthens the existing exemplar. If a token of experience differs from the existing exemplars in some dimension, it may either fade away or, if reinforced by later experiences, form the basis for a new exemplar. Similar exemplars are stored near one another to constitute clusters or categories.\footnote{\citet[127]{sherwood-fundamentals-2012} explains the fact that long-term memory traces are stored with memories of the same type by the need to maintain  memory stores searchable for future retrievals.} In Johnson's (\citeyear[][146]{johnson97}) terms, categorisation is based on ``sums of similarity over each category''. This implies that a single usage event, or a token of experience, affects multiple exemplar clusters. In a nutshell, the exemplar model holds that every token of experience, or use, affects cognitive representation \citep[cf.][]{bybee-beckner-2009}, although it should be kept in mind that ``an individual exemplar -- which is a detailed perceptual memory -- does not correspond to a single perceptual experience, but rather to an equivalence class of perceptual experiences'' \citep[141]{pierrehumbert-2001}. 

Exemplar models were developed to capture similarity and frequency in perception \citep[cf.][]{pierrehumbert-2002}. However, already \citet{johnson97} articulated the idea that an exemplar model is capable of explaining the production-per\-cep\-tion link. He argues that exemplars include not only auditory properties, but also articulatory properties. According to \citet{pierrehumbert-2002}, a goal for the current production corresponds to the average properties of the exemplars in a cluster. In order to produce a certain linguistic item, activation spreads to a group of exemplars in an area of the perceptual map. Hence, this type of model can successfully account for the effects of language exposure on production.

One of the consequences of modelling linguistic representations in terms of exemplars is that ``[e]xemplar representations provide a natural way to allow frequency of use to determine the strength of exemplars'' \citep[717]{bybee-repetition-2006}. The idea that usage events, which are inherently repetitive, can shape the cognitive representation of language is relatively recent in modern linguistics, but it was upheld as early as 1880 by Hermann Paul in his \textit{Principles of the History of Language} \citep[cf.][]{auer-paul}. The following passage from Paul conveys the gist of his version of an exemplar model:

\begin{quote}
[\dots] every act of speaking, listening, or thinking adds something new. Even by an exact repetition of a previous act, some forces of the already existing mental grammar are strengthened. And even if somebody can look back on a rich linguistic life there is always the opportunity for something new: Aside from the introduction of things that were previously unusual in the language, at least a new variant to old elements may be added. [\dots B]oth the attenuation and the strengthening of the old elements as well as the addition of new ones constantly shift the associations within this system of representations. (Quoted in \citealt[31]{auer-etal-paul})
\end{quote}

\noindent We can conclude from the above that the perception-production feedback loop enables the system to capture the dynamic character of experience by constantly updating the existing exemplars. Another idea contained in the passage concerns the fact that, if not reinforced, linguistic memories, just as non-linguistic memories (\citealt[127]{sherwood-fundamentals-2012}), may decay  \citep{pierrehumbert-2001} and thereby trigger a reorganisation of the network \citep[cf.][]{bybee-beckner-2009}. This scenario is particularly common in a situation of an ongoing language change \citep[718]{bybee-repetition-2006}.

In an exemplar model, a word in a speaker's lexicon corresponds to a cluster, or cloud, of phonetic exemplars which represent the word's phonetic variants with information about their linguistic and social contexts. The meaning of the word is stored in a cluster of semantic exemplars which represent information about the meaning and context of each token of a word. Together the cloud of phonetic exemplars and the cluster of semantic exemplars are considered a unit \citep[for an overview and further details, see][]{bybee-beckner-2009, bybee-book-2010}. \citet{pierrehumbert-2001} points out that the same remembered tokens may be categorised according to several schemes. This idea can be illustrated by the phrase \textit{Stand clear of the closing doors, please}, whose recollection can involve such categories as the words and phonemes in the phrase, `underground rapid transit' and, in some speakers, `male voice' and `the New York City Subway'.

Usage-based models assume that words and even word strings are linked to other words and word strings. These relations arise from similarities among words along phonetic and semantic dimensions (\citealt[101]{kruszewski} [1883]), and are represented in networks of exemplars. The exemplar space, or networks, naturally provide the capability for generalisation based on similarity \citep[cf.][]{pierrehumbert-2002}. Generalisations emerge on multiple levels. For example, \citet{bybee-morphology-1985, bybee-book-2010, bybee-word-2002} argues that both morphological relations and morphemes arise from similarity relations among words grounded in shared semantic and phonetic features. Specifically, she shows that the internal structure of a word, such as \textit{revitalise}, is a result of its relations with other words, including \textit{vitalise}, \textit{revitalisation}, \textit{generalise}, \textit{vital}, \textit{redo} and the like. 

Above the word level, word strings are also represented in a network of relations. Storage is required for word strings which exhibit idiosyncrasies of meaning, such as \textit{red herring} or \textit{make waves}, and those which are compositional in form and meaning but represent conventionalised expressions, such as \textit{happy birthday} -- and not \textit{joyous birthday} -- or \textit{take a decision} -- and not \textit{seize a decision}. In the former case, the necessity to register the string in memory arises from the unpredictability of its meaning, whereas in the latter case, it results from the use of a string as a conventional sign for a concept. By virtue of a network representation, a speaker may retrieve the sequence as a whole and yet maintain associations between the sequence and its component parts as independent words, i.e., the words \textit{red}, \textit{herring}, \textit{make} and \textit{waves} in the respective examples \citep[cf.][25]{bybee-book-2010}. 

With regard to the storage of complex words and word strings, a usage-based approach is not concerned with the question whether a complex unit is represented or not, it rather handles questions concerning ``the strength of the representation and the strength of its association with other representations, both paradigmatic and syntagmatic, all of which are variable'' (\citealt[24]{bybee-book-2010}; cf. also \citealt{lieven-2010}). The strengths of representations and associations, corresponding to particular units, reflect a language user's experience with these units and has been found to correlate with the (frequency) distributions of these units in her usage. Specifically, the strength of representation of a linguistic unit, often referred to as “entrenchment” \citep{langacker-foundations-1987,croft-2001,tomasello-constructing-2003,blumenthal}, depends on the token frequency of this structure, or its frequency of occurrence. There is a general agreement among usage-based linguists (e.g., \citealt[59]{langacker-foundations-1987}; \citealt[581]{bybee-scheibman}; \citealt[106--107]{tomasello-constructing-2003}; \citealt[283]{bybee-frequency-2007}; \citealt[68]{blumenthal}) that repetitive sequences are entrenched to become units. However, even more important is the acknowledgement that unit status is a matter of degree, since a sharp distinction between units and non-units is psychologically implausible \citep[59]{langacker-foundations-1987}.

As stated above, storing exemplars of words and word sequences which are similar on one or several dimensions in close proximity to each other in a network enables the emergence of exemplar clusters, or categories. The network organisation has as a consequence that exemplar clusters develop association relations on the basis of similarity and co-activation patterns. These association relations in the exemplar space underlie generalisations over exemplar clusters. Usage-based theories hold that the network organisation of the exemplar space gives rise to representation of abstract structure, including phonological, morphological and syntactic structure. It must be noted however that the emergence of abstract knowledge in consequence of generalisation over specific items still needs further empirical support and theoretical elaboration. Below I will exemplify a possible path of emergence of such structure with a case of a syntactic construction.

Usage-based linguistics regards constructions, defined as ``direct form-mean\-ing parings that range from the very specific (words and idioms) to the more general (passive construction, ditransitive construction), and from very small units (words with affixes, \textit{walked}) to clause-level and even discourse-level units'' (The “Five Graces Group” (\citeyear[5]{five-graces}), cf. \citealt{croft-2001, goldberg-2003, goldberg-2006}), as basic units of grammar.

Usage-based analyses (e.g., \citealt{boas-2003, lieven-etal-2003, goldberg-etal-2004, dabrowska-lieven-2005, bybee-eddington, boyd-goldberg-2011}) have shown that when similar words differing in some aspect appear in the otherwise same sequence in the input, exemplar categories for both the invariable and varying items emerge. While the group of varying exemplars represents a schematic position in a construction, the group of exemplars corresponding to the invariable elements form the fixed slots in a construction. For example, the word sequences in (\ref{ex:2:1}) have the word \textit{hour} as a fixed element, whereas the words adjacent to it alternate: the items in the initial position are numbers and the items in the final position denote various acts of travelling from one place to another.

\ea
	\label{ex:2:1}
	\textit{thirty}-hour \textit{ride}\\
	\textit{half}-hour \textit{drive}\\
	\textit{four}-hour \textit{flight}\\
	\textit{two}-hour \textit{trip}\\
	\textit{three}-hour \textit{journey}\\
	\textit{two}-hour \textit{hop}\\
	\textit{three}-hour \textit{slog}\hfill\hbox{\citep[data from][16--17]{hoey-lexical-2005}}
\z

\noindent The range of nouns that occur in the final position is limited, and the semantic category they belong to is constrained: they all refer to some act of travelling. My analysis of these nouns in the British National Corpus (BNC, \mbox{\citealt{bnc})} revealed that they occur with the word \textit{-hour} with the following frequencies: \textit{journey} (35), \textit{drive} (26), \textit{flight} (26), \textit{trip} (12), \textit{ride} (4), \textit{hop} (0), \textit{slog} (0). We could conclude from these figures that the word \textit{hour} combines with the word \textit{journey} more often than with the other words. Following \citet[][81]{bybee-book-2010}, we can regard the sequence \textit{-hour journey} as a prefab, which is defined as a highly frequent exemplar that ``represents the conventional way of expressing an idea''. Prefabs provide the basis for the emergence of semantic categories. Semantic restrictions apply to even more schematic categories \citep[cf.][81]{bybee-book-2010}, such as the category constituted by the words occupying the first position in the examined sequences, all of which are numbers. The construction that arises from the specific instances in (\ref{ex:2:1}) in the process of generalisation could be described as \textsc{number}-\textit{hour} \textsc{act-of-travelling}, where the fixed slot is surrounded by two schematic slots (it is noteworthy that both slots exhibit differing degrees of schematicity). Even the word occurring in the second position in the examined sequences may alternate. Possible alternatives include \textit{day}, \textit{week}, \textit{month} and \textit{year}. Together with the word \textit{hour}, they form the schematic category \textsc{time-unit}. A broader generalisation results in the construction \textsc{number-time-unit act-of-travelling}. 

As has been shown above, under the exemplar model the representation of a construction, which may be a fairly broad generalisation, is linked to the exemplars of all specific words experienced in a certain position in that construction. These item-general links may be maintained even when specific word sequences instantiating these constructions grammaticalise \citep{torres-walker-2009}. Importantly, the view of constructions as generalisations over specific exemplars does not mean that items previously unexperienced in a particular construction may not be used with it in production. On the contrary, constructions are potentially productive \citep{goldberg-2006,lieven-2010}. Their productivity depends on the  semantic -- and sometimes even formal -- specifications of their schematic slots (\citealt[57--59]{bybee-cxg-2013}; see also \citealt{zeschel-2010}). In other words, a speaker may extend the use of a construction to new items if their semantic properties are compatible with the semantic properties of the words that support the corresponding generalisation. To illustrate this point, I again refer to the \textsc{number-time-unit act-of-travelling} construction. \citet{hoey-lexical-2005} argues that thanks to this generalisation (his term is ``semantic association''), language users can produce and interpret such sequences as \textit{27-hour meander}, \textit{27-week flight} and \textit{multi-month odyssey}, although they may have never experienced them. The fact that the knowledge of language encompasses not only simplex words but also schematic constructions, their specific instances and combinations of schematic and concrete pieces of language means that language elements are simultaneously represented in the mind at various degrees of granularity (\citealt{goldberg-2006,bybee-repetition-2006,tomasello-constructing-2003}; \citealt[see][63--76 for the view of grammar as a structured inventory of linguistic units]{langacker-foundations-1987}). This means that a language user may represent a given multiword sequence or multimorphemic word in her mind as a concrete expression and as an instantiation of some more schematic construction or constructions. 

To summarise, an exemplar model posits that linguistic structure is represented in the language user's mind as memories of specific language experiences, which are organised as exemplars, and simultaneously in the form of generalisations over these memories. Exemplars of concrete linguistic expressions are stored in clusters. Newly experienced instances of language are subject to categorisation. If they correspond to any of the existing exemplars, they exert an accumulative effect on this exemplar. If they are not identical but similar to one or several existing exemplars, they are stored in close proximity to them and may form a new cluster. Exemplar clusters are organised in networks on the basis of various associations between them. These associations are due to either simultaneous activation or similarity. Schematic constructions may emerge from these networks as generalisations over exemplars representing concrete items. A linguistic expression may be simultaneously represented in the language user's mind at multiple levels of granularity. Namely, the language user may retain the memory of the expression as a whole, the voice that pronounced it and its intonation pattern, but also the situation in which the expression was uttered. Furthermore, she is most likely to activate and possibly reinforce the exemplars corresponding to the components of this expression. These components include specific items -- i.e., chunks, collocations and single words -- and instantiations of more schematic syntactic and morphological constructions as well as the phonetic features and phonological patterns underlying single words and longer word sequences. Representing structures of varying degrees of specificity/abstractness is possible because different processes are involved; namely, representations of specific items rely on patterns of co-activation in language use, whereas the representation of abstract structure emerges in consequence of similarity detection and categorisation.

One of the consequences of an exemplar model is that it relates the strength of a mental representation directly to language use. In order to be able to store linguistic structure, an individual has to encounter and use it on a regular basis. This is of particular relevance to multimorphemic elements since the language user can only remember those strings that appear in the input frequently enough. In the next section, I will show that while some multimorphemic elements may be memorised as unanalysed holistic amalgams in the process of language acquisition, representations of other strings may emerge in the mind through chunking, whereby the existing representations of smaller linguistic items corresponding to parts of frequently experienced strings are repeatedly co-activated in sequence to give rise to larger processing units representing the respective sequential information. I will first describe the learning of multimorphemic elements by rote in first-language acquisition and will then elaborate on the process of chunking as the other source for representing these structures in the mind.

\section{Recurrent multimorphemic elements in language acquisition and use}

A body of evidence has emerged in the last two decades suggesting that everyday language involves a wide array of fixed multiword, or formulaic, sequences (e.g., \citealt{corrigan-etal-2009,schmitt2004,wray-2002,wray-2008,taylor-mental-2012}). However, this idea was already expressed as early as 1974 by Bolinger.\footnote{According to \citet{tremblay-etal11} the idea of the unintentional fusion of two or more linguistic signs into a single unit goes back to Ferdinand de Saussure (1959 [1916], quoted in \citealt[571]{tremblay-etal11}), for whom fusion was a particular type of agglutination.} He asserted, ``[s]peakers do at least as much remembering as they do putting together'' (1976; quoted in \citealt[29]{erman-warren-2000}), and argued -- in contrast to the views then current -- that the language user stores a large number of complex items. \citet{sinclair-1991} cast the same idea into the idiom principle and the open-choice principle. He defines the idiom principle in the following way:

\begin{quote}
The principle of idiom is that a language user has available to him or her a large number of semi-preconstructed phrases that constitute single choices, even though they might appear to be analysable into segments. \citep[][110]{sinclair-1991}
\end{quote}

\noindent The idiom principle is contrasted with the open-choice principle, positing that almost every position in the syntactic structure allows an open choice, i.e., virtually any word can fill a slot. Sinclair relates the open-choice principle to the so-called ``slot-and-filler models'' of grammar (p. 109), such as traditional structuralist (and generative) grammars. These models, according to \citet{pawley-syder-1983}, cannot account for native-like, i.e., idiomatic, selection and native-like fluency in spontaneous conversation. The authors emphasise that speakers should store abundant units of clause length (``lexicalized sentence stems'' in their terminology) in order to minimise ``the amount of clause-internal encoding work [\dots and] to attend to other tasks in talk-exchange, including the planning of larger units of discourse'' \citep[][92]{pawley-syder-1983}. Although the ideas expressed by Bolinger (1976), \citet{pawley-syder-1983}, and \citet{sinclair-1991} lack explicit links to usage-based accounts of language, they are highly compatible with the usage-based view of grammar \citep[cf.][]{five-graces}. For example, one of the possible factors that \citet[][110]{sinclair-1991} considers responsible for the emergence of the idiom principle is ``the recurrence of similar situations in human affairs'', which is obviously a usage-based motivation.\footnote{The other two motivations suggested by Sinclair are ``a natural tendency to economy of effort'', and  ``the exigencies of real-time conversation'' (ibid.: 110).} And \citet[][208]{pawley-syder-1983} point out that a large proportion of fluent stretches in the conversations that they analyse are familiar, memorized clauses and clause-sequences. This is in line with the usage-based tenet that a human being's memory for language is rich.

The observation that large portions of language are available to language users as prefabricated, or formulaic, units has stimulated research into the nature of these units and their usage. Extensive studies of formulaic expressions have been conducted in corpus linguistics and applied linguistics. Whereas studies of spoken language investigate conversational routines \citep{aijmer-1996,altenberg-1990,altenberg-1998}, analyses of large corpora of written texts often centre around the identification of prefabricated phrases \citep[e.g.,][]{erman-warren-2000,gries-2010,evert-2005} and their use in various registers \citep[e.g.,][]{biber-conrad-1999,gries-mukh-2010}. Prefabricated phrases are naturally of particular relevance to linguists interested in second language acquisition \citep[e.g.,][]{granger-1998,ellis-etal-2008,biber-etal-2004,schmitt2004}. And recently, psycholinguistic studies have gathered considerable evidence that language users -- both children and adults -- have allocated representations for strings of concrete linguistic structures and heavily rely on them in language comprehension and production. 

A central tenet of the usage-based approach to multiword sequences and multimorphemic words is that, given a sufficient frequency in a language user's experience, a complex expression tends to be entrenched in her mind so that it can be easily accessed and fluently executed. Some sequences are learnt as holistic units already during  language acquisition, while other sequences may be chunked (i.e., gain an independent representation) later, as experience with language grows. 

Following these observations, the next section first reports findings from the literature on the use of multimorphemic elements by children learning a language, then it introduces chunking as a principle of memory organisation, briefly indicating some of its consequences for language organisation and use. The final sub-section outlines some pertinent comprehension and production studies investigating the processing of multimorphemic elements in adults. 

\subsection{Recurrent multimorphemic elements in first-language acquisition}
A continuous strand of research from as early as 1970s suggests that language development begins at a very concrete level (see in particular \citealt{bowerman-1973,clark-1974,braine-1976,tomasello-1992,pine-lieven-1993,dabrowska-lieven-2005}). Considerable evidence has been adduced to date that language acquisition begins with learning multiword and multimorphemic adult expressions as unanalysed units \citep{lieven-pine-baldwin-1997,dabrowska-2004,bannard-matthews-2008,lieven-salomo-tomasello-2009,arnon-clark-2011}. One reason why children store more than individual words in their memory is that ``they do not hear demarcated words in the input; words and phrases run into one another and must be detected in the speech stream'' \citep[][241]{bannard-matthews-2008}.

In a diary study of his daughter's speech, Michael \citet{tomasello-1992} observes that her first word combinations correspond to unanalysed adult expressions. He finds that one of the earliest multiword expressions in her speech is \textit{whereda-bottle}, which the child produced at the age of around 15 months, aiming at the adult expression \textit{Where is the bottle?} (ibid.: 45). Another example is the expression \textit{get-it}, which the child started to use around two months later to request objects that were in sight but inaccessible (ibid.: 72--73). This expression is again modelled on her parents' specific constructions, including \textit{I'll get it}, \textit{Go get it} and \textit{You can get it}. But unlike wholly rote-learnt utterances such as \textit{whereda-bottle}, this expression draws only on the invariable parts of the frequently experienced constructions. Tomasello refers to this type of expressions as limited-scope formulae (ibid.: 22).

In a series of corpus studies, Elena Lieven and her colleagues \citep{lieven-pine-baldwin-1997,dabrowska-lieven-2005,lieven-salomo-tomasello-2009} demonstrate that productivity in young children's utterances is limited, and up to a half of children's first 300 multiword utterances represent fixed strings. The authors conclude that children are learning both single words and word strings, which are learnt as ``big words''. Such word strings are reported to exhibit a high degree of repetitiveness in the input and are therefore highly learnable. While learnt word strings are usually subject to a subsequent internal analysis, sufficiently entrenched strings may remain in the memory unanalysed and be represented as fully lexically specific strings. Experimental evidence in support of this latter claim has been obtained by \citet{bannard-matthews-2008}, who tested young children's memory for familiar word sequences. The authors extracted frequently-occurring chunks in a corpus of child-directed speech, such as \textit{sit in your chair}, and matched them to infrequent sequences, e.g., \textit{sit in your truck}. Preschoolers' ability to produce these sequences was tested in a sentence repetition task. Two and three-year-olds were significantly more likely to correctly repeat frequent sequences than infrequent sequences. The authors conclude that children have allocated representations for sequences of more than one or two words. 

Inbal Arnon (\citeyear{arnon-2011}) argues that children learn word sequences not only because the latter are highly repetitive in the input, but also because children naturally attend to larger phrases. She regards word strings as good candidates for a Gestalt process, defined as ``the move from large unanalyzed units to the identification and analysis of smaller more structured ones'' \citep[][167]{arnon-2011}. She provides evidence for this claim in a experimental study conducted jointly with Eve Clark (\citeyear{arnon-clark-2011}). This work investigates four to six-year-olds' production of irregular plurals in English, depending on the context in which the irregulars occurred. The given context involved either a lexically specific frequent frame, e.g., \textit{Three blind  -- } or a general question \textit{What are these?} The authors found that children produced 72 per cent of correct irregular plurals after lexically specific frequent frames and only 32 per cent after a general question. These findings imply that children represent sequences of words, which reflect their language use.

With regard to sequences of lexical and grammatical morphemes, such as inflected forms, substantial evidence has been presented that young children learn them as unanalysed amalgams (see \citealt[][118--119]{tomasello-constructing-2003}; and \citealt[190--193]{clark-2009}, for reviews). For example, \citet{pizzuto-caselli-1992} observe this strategy in three Italian-speaking children (from 1;4 to 3;0 years of age) learning the verbal morphology. Specifically, they find that of six possible inflected forms, the children used only one form with 47 per cent of all verbs, and two or three forms with 40 per cent, while four or more forms were observed only with 13 per cent of all verbs, which included highly frequent, irregular verbs, learnt by rote. One implication of these findings is that children at this age do not use inflectional suffixes productively, but rather learn combinations of specific verbal stems with particular inflectional suffixes. In other words, before abstracting the verb paradigm children start storing those inflected forms whose frequency in the input is high. 
 
\begin{sloppypar}
In the domain of nominal inflection, \citet{dabrowska-2004} shows that in the speech of Polish-learning children, the first multimorphemic forms consisting of a stem and an inflectional suffix of the genitive case appear only with particular lexical items before they begin to be used productively with other lexical items some six months later. A similar situation is observed by \citet{pine-lieven-1997} in young children learning English determiners. The authors demonstrate that specific determiners in children's utterances are closely associated with particular nouns. This result is interpreted as evidence that children store in their lexicon/grammar specific sequences consisting of determiners and nouns before they develop the category of determiner. 
\end{sloppypar}

Putting the matter in a nutshell, studies of child language have amassed indications that child language acquisition involves rote learning of many highly specific multimorphemic words and word combinations. However, as pointed out above, representations of such structures may also emerge in language use as a consequence of chunking.

\subsection{Chunking and the mental representation of multiword sequences and multimorphemic words}\label{chunking}
Chunking is the cognitive mechanism whereby permanent sets of associative connections are developed in the long-term memory \citep[][]{ellis-1996}. This mechanism is responsible for a fast and fluent performance of sequenced actions \citep{melton-1963}. With increasing practice, or frequency, sequences of actions are performed faster because the sequences are processed as units \citep{miller-1956,anderson-1982}. \citet{newell-1990} regards chunking as the overarching principle of human cognition:

\begin{quote}
A chunk is a unit of memory organization, formed by bringing together a set of already formed chunks in memory and welding them together into a larger unit. Chunking implies the ability to build up such structures recursively, this leading to a hierarchical organization of memory. Chunking appears to be a ubiquitous feature of human memory. Conceivably, it could form the basis for an equally ubiquitous law of practice. \citep[][7]{newell-1990}
\end{quote}
He points out that people chunk at a constant rate: Every time they get more experience, they build additional chunks. 

The observation that new behavioural routines emerge as experience grows has been applied to language only recently. Researchers who argue for the relevance of chunking to language are Nick Ellis (\citeyear{ellis-1996}), Joan Bybee (\citeyear{bybee-constituency-2002}), and Christiansen and Chater (\citeyear{christiansen-chater-2016}). Their proposal is that the frequency with which linguistic units are perceived, or produced, in sequence strengthens associations between them in the long-term memory. Another mechanism is suggested by \citet{diessel-toappear}, who argues that the process behind the emergence of units corresponding to sequential elements is automatisation, which is closely related but not identical to chunking. He describes automatisation in the following way: 

\begin{quote}
Automatization is the cognitive mechanism whereby controlled processes are transformed into automatic processes. Almost all sequential activities start off as controlled processes, but are then often transformed into automatic processes through repetition or practice. This is a very common cognitive phenomenon involved in many everyday tasks. Automatization enables people to perform complex sequential activities with little effort [\dots] \citep[][19]{diessel-toappear}
\end{quote}

\noindent It appears that chunking and automatisation are interrelated as both refer to aspects of complex sequential activities. \citet[][20]{diessel-toappear} asserts that these processes complement each other; namely, automatisation is the process responsible for the processing of sequential elements and their transformation to units, whereas chunking is the process behind their storage and organisation in memory. However, whether it is beneficial to contrast these complementary processes is a practical and theoretical question. On the one hand, the distinction between the procedural and representational aspects of sequential actions may provide us with additional insight into specific properties of these processes.\footnote{\citet{diessel-toappear} notes that chunking and automatisation are not always distinguished in linguistic literature.} On the other hand, if we adopt \citeauthor{melton-1963}'s (\citeyear[][]{melton-1963}) conception of memory as a continuum of short-term memory and long-term memory, we have to conclude that a separation of these processes is problematic because every execution of sequential actions leads not only to automaticity in performance but also irresistibly strengthens the corresponding trace in the long-term memory (\citealt[cf.][1046]{hay-2001}). For this reason I will not treat chunking and automatisation as essentially different processes in this work.\footnote{Although \citet{bybee-constituency-2002} does not define chunking and automatisation (“automation” in her terminology) explicitly, her treatment of these processes rests on their inseparability, as is reflected in her account of fluent production of recurrent word combinations: ``[\dots] repeated sequences become fluent because they become automated into a single chunk that can be accessed and executed as a unit'' (ibid.: 316).}

In the domain of second-language acquisition, \citet{ellis-1996} argues that language learning is concerned with the acquisition of memorised sequences of language on various levels of linguistic structure. He reviews experimental studies documenting a reciprocal relationship between short-term memory and long-term memory in language learning and observes that short-term representation and repetition are responsible for the development of long-term sequence information, whereas long-term sequence representations enable the chunking of working memory contents \citep[][115]{ellis-1996}. The latter circumstance could be interpreted as the basis for the development of hierarchical relations between elements in the process of learning. 

The emergence of hierarchical relations in syntax is the focus of \citet{bybee-constituency-2002}. The author reports evidence from the literature that frequently repeated word combinations undergo fusion, or are chunked, sometimes in violation of traditional notions of constituent structure. This process is captured by the Linear Fusion Hypothesis: ``Items that are used together fuse together'' (ibid.: 112; see also \citealt{bybee-scheibman}). In a study conducted in a corpus of spoken American English, Bybee observes a general correspondence between patterns of sequential co-occurrence and constituent structure in the domain of the noun phrase and comes to the conclusion that ``the hierarchical structure of language is derivable from the more basic sequential nature of language''. 

In a similar vein, but in an more general perspective, integrating language evolution, acquisition and processing, \citet[][15]{christiansen-chater-2016} view chunking as an essential tool for processing language because it allows the brain to “compress and record its input into “chunks” as rapidly as possible” and to pass it to a higher level of representation. Listeners have to act in this way, as Christiansen and Charter argue, because they are confronted with a massive flow of linguistic information in face-to-face interaction and memory is limited at every level of representation. In production, the process is reversed as the memory constraints require that “only a few chunks are kept in memory at any given level of linguistic representation“ \citep[][102]{christiansen-chater-2016}. Crucially, this model directly links chunking and the multiple levels of linguistic representations in the mind. In other words, chunking is fundamental to language processing and thus to the organisation of language.

To summarise, individual items processed in sequence are grouped together into chunks. With increasing practice, the associative links between the representations of the individual items become stronger, so that the sequence can be accessed and executed as a unit. The process of chunking, whereby such sequential units emerge, is crucial not only to fluency and the fusion of sequential elements but also to hierarchical relations in language. The following section reviews studies which investigate the processing of recurrent multimorphemic words and multiword sequences and thus presents  compelling evidence for their psychological reality.  

\subsection{Processing of multimorphemic elements}
\label{processing}

\subsubsection{Comprehension studies}
The impact of usage frequency on the processing of multimorphemic words and multiword sequences has been the focus of almost four decades of psycholinguistic research. Starting from the early 1970s, experimental psycholinguistics has richly documented a facilitatory effect of lexical frequency on the recognition of multimorphemic, or complex, words, but it is only recently that researchers have developed an interest in the processing of multiword sequences (this is probably due to the late rise of corpus-based studies). Generally speaking, psycholinguistic studies confirm the observation that both frequent multimorphemic words and frequent multiword sequences are processed in a different way than their less frequent counterparts. 

Early studies of lexical access to multimorphemic words, which include inflected, derived and compound words, report a correlation between the speed of access to a multimorphemic word and its usage frequency (\citealt{morton-1969, taft-forster-1976, bradley-1979, stemberger-macwhinney-1986}; see also \citealt{giraudo-grainger, janssen-bi-caramazza} as examples of some recent studies). But already \citet{taft-1979} presents evidence that the recognition time for multimorphemic words is sensitive to both the word's surface frequency and the frequency of the word's base (cf. \citealt{burani-caramazza-1987,cole-etal-1989,alegre-gordon-1999,meunier-segui}). To account for the competition between a complex word and its base morpheme in lexical access, many current models of morphological representation postulate two access routes for multimorphemic words: a decomposed route and a non-decomposed route. In the latter case, a complex word is accessed supralexically, i.e., directly, whereas in the former case, access is mediated by contact to the decomposed parts of the word, its base in the first place (cf. \citealt{caramazza-etal-1988,baayen-1992,frauenfelder-schreuder-1992,baayen-schreuder-1999,hay-2001,blumenthal}; but see \citealt{bien-baayen-levelt} for an alternative approach). These models cogently accommodate the effect of a word's surface frequency by stipulating that holistic storage of a multimorphemic word is a function of its surface frequency relative to the frequency of its base. In other words, the higher the frequency of the multimorphemic word relative to the frequency of its base, the higher the likelihood for this word to be accessed directly and to have an allocated autonomous representation in the mental lexicon. Most studies investigating lexical access to multimorphemic words provide support for this frequency effect, even though the overall reported results appear somewhat inconclusive. This confusion owes in part to differences in the types and functions of the analysed morphemes (e.g., inflectional vs. derivational suffixes), the typological differences between the investigated languages, but also, as \citet[][1063--1066]{hay-2001} notes, some methodological caveats. 

To give an example, several studies which investigate processing of singulars and plurals\footnote{I report the findings on the processing of singulars and plurals here because they are relevant for Chapter \ref{PL}, which explores the use of German plurals in otherwise Russian sentences.} by using lexical decision tasks present conflicting evidence for English \citep{sereno-jongman-1997,new-etal-2004}, on the one hand, and Dutch \citep{baayen-dijkstra-schreuder} and French \citep{new-etal-2004}, on the other hand. Whereas in all languages the effect of surface frequency holds for plural forms of plural-dominant words, i.e., words whose plurals are more frequent than singulars, the processing of singular forms behaves according to the surface frequency in English \citep{sereno-jongman-1997,new-etal-2004}, but neither in Dutch \citep{baayen-dijkstra-schreuder} nor French \citep{new-etal-2004}, where it predominantly depends on the base frequency. The competition between the base and the whole inflected word was interpreted as an indication of parallel activation of both the storage route and the decomposed route.

Findings from studies which explore the processing of multiword sequences are more straightforward, although all of them are based on English data and the examined multiword sequences vary in length, structure and complexity. This research relies on diverse on-line methods such as the phrasal decision (or recognition) task \citep{bod2000,arnon-snider}, the word monitoring task \citep{sosa-macfarlane,kapatsinski-radicke}, the self-paced reading paradigm \citep{macdonald,reali-christiansen,bannard-ramscar,tremblay-etal2009,tremblay-etal11} and eye-tracking methods \citep{mcdonald-shillcock,siyanova-etal}.

According to \citet[51]{jurafsky2003}, \citet{macdonald} was one of the first to show that the frequency with which two words appear together influences the reading time of these words in sequence.\footnote{Another study which Jurafsky mentions is \citet{trueswell-etal}.} She used a self-paced reading task to investigate the factors that determine the reading time of sequences consisting of nouns and words ambiguous between nouns and verbs, such as \textit{stores}. The interpretation of the second word as a noun is likely to be evoked if the noun-noun pair is frequent, such as \textit{grocery stores}, but not in the case of infrequent noun-noun pairs, such as \textit{warehouse fires}. The finding that an inverse relationship exists between the frequency of a word-pair (bigram) and its recognition was extended by \citet{bod2000} to three-word sequences. He demonstrated in a phrasal decision experiment that frequent three-word sequences, which corresponded to subject-verb-object sentences, such as \textit{I like it}, were recognised faster than their infrequent counterparts, e.g., \textit{I keep it}, when other factors including word frequency, word complexity, syntactic structure and plausibility are controlled. A reason for this result, as pointed out by \citet[62]{jurafsky2003}, may be a lack of control over sub-string bi-gram frequencies. Interestingly, \citet{bannard-ramscar} could replicate Bod's (\citeyear{bod2000}) result for sequences of between four and seven words under control of the aforementioned factors as well as sub-string bi-gram frequencies.

I will not give a concise account of recent research but only briefly raise the point that the multiword sequences examined in the reported studies vary in terms of their structure. For instance, \citet{reali-christiansen} and \citet{arnon-snider} as well as the aforementioned studies investigate the recognition of multiword sequences which exhibit an overlap with the phrase structure. The examined sequences include such items as \textit{a lot of places} and \textit{I have to pay} in Arnon \& Snider's (\citeyear{arnon-snider}) investigation, and \textit{I}-verb chunks, e.g., \textit{I met} and \textit{I liked}, in Reali \& Christiansen's (\citeyear{reali-christiansen}) work. The former study explores the processing of four-word sequences in isolation, whereas the latter study investigates the comprehension of pronominal object relative clauses, of which the target pronoun-verb combinations are part. Following \citet{tremblay-baayen}, I refer to multiword sequences of this kind as phrasal multiword sequences. The processing of non-phrasal multiword sequences is the focus of \citet{tremblay-etal11}. The authors adopt Biber et al.'s (\citeyear{biber-etal-1999}) term “lexical bundle” to refer to multiword strings: ``A lexical bundle (LB) is a relatively common continuous multiword sequence that may span phrasal boundaries'' \citet[][572]{tremblay-etal11}. Although the definition implies that a lexical bundle crosses syntactic boundaries only optionally, many of the items examined by \citeauthor{tremblay-etal11} do not match the phrase structure, e.g., \textit{in the middle of the}. We may therefore refer to them as non-phrasal multiword sequences. It should be noted, however, that each of the studied items was embedded in a sentential context of two words to the left and to the right, e.g., \textit{I sat \textbf{in the middle of the} bullet train}. Unfortunately, no comprehension studies are known to me which pursue a comparison between phrasal and non-phrasal multiword sequences as well as their processing in isolation versus in a context.

Another aspect of multiword sequences that I would like to highlight is the competition between sequences forming linguistic units and their parts. \citet{kapatsinski-radicke} argue that although a high-frequency multiword sequence seems to form a linguistic unit of its own, access to it may be mediated by competition with its parts. The authors conducted a monitoring experiment in which participants had to respond whenever they detected the particle \textit{up} in a verb-particle combination such as \textit{get up}. Reaction times sped up with the frequency of the verb-particle collocation increasing. But the particle detection decelerated when the collocation frequencies reached the highest values. This result confirms Sosa and MacFarlane's (\citeyear{sosa-macfarlane}) finding that detection of \textit{of} was slower in collocations in the highest frequency bin such as \textit{kind of}. \citeauthor{kapatsinski-radicke} interpret these results as evidence for the storage of only high-frequency phrases as units in the lexicon and the competition between the part and the whole for collocations which are not stored in the lexicon. In such sequences, detectability of the particle improves with its predictability growing, given the sequence. These findings are reminiscent of the outlined parallel activation account of multimorphemic words, which posits a competition between the storage route and the decomposed route \citep{baayen-dijkstra-schreuder,hay-2001}.

In sum, despite the indicated differences in the methodology, the utilized material, and the theoretical interpretations, all the outlined studies suggest that regular multiword sequences, whether phrasal or non-phrasal, leave memory traces in the brain. This result is consistent with the evidence laid out above that regular multimorphemic words, whether inflected, derived, or compounded, may be stored in memory holistically.

\subsubsection{Production studies}\largerpage

This section outlines psycholinguistic research which established the influence of frequency on the production of multimorphemic words and multiword sequences. In each of these domains, I will first present some relevant findings from acoustic-phonetic corpus studies and then provide a brief outline of the most relevant experimental research. 

Studies which investigate the production of multimorphemic words in spoken language corpora are based on the observation that lexical frequency determines the production of words in spontaneous speech. As a rule, high-frequency words are subject to phonetic reduction \citep{bybee2000} and tend to be pronounced shorter than low-frequency words \citep{jurafsky-etal2001}. Studies focusing on the production of complex, multimorphemic words not only aim to pinpoint factors responsible for phonetic reduction, but also to scrutinise the (possible) consequences of this process for the morphological structure of words. 

For example, \citet{keune-etal2005} examine the  variation in the reduction of Dutch words with the suffix \textit{-lijk} such as \textit{natuurlijk} `of course', \textit{moeilijk} `difficult' and \textit{uiteindelijk} `finally', and attribute it to socio-geographic and linguistic factors. The analysis of these multimorphemic words in a corpus of spoken Dutch reveals that the degree of reduction is the largest for the high-frequency words \textit{natuurlijk} `of course',  \textit{mogelijk} `possible' and \textit{eigenlijk} `actually', whose reduced forms are [tyk], [mok], [ɛɪk], respectively.\footnote{\citet{keune-etal2005} emphasise that in addition to these highly reduced forms, other, less reduced forms also exist.} The authors conclude that these words undergo a process of erosion, which is marked by a loss of morphological structure and a development towards monosyllabic forms. Alongside frequency, the variation in the reduction of words ending in \textit{-lijk} is explained as an interplay among such factors as socio-geographic origin, speech rate, the word's position in the sentence and its contextual predictability, measured by mutual information, which is a frequency-based probability measure of the likelihood that a word would occur given the preceding word. \citeauthor{schaefer2014}'s (2014) analysis of Icelandic adverbs with the suffix \textit{-lega} confirms \citeauthor{keune-etal2005}'s result. The main finding of his work is the effect of lexical frequency on the phonetic reduction of adverbs in \textit{-lega}, in the absence of any effect on the part of metrical rhythm. 

\citet{bergmann2012} uses electropalatography, an articulation-based method, to investigate the influence of lexical frequency and prosodic structure on articulatory reduction of n\#g-sequences in German compounds such as \textit{Zinn\#krie-ger} `tin warrior' and particle verbs, e.g., \textit{ein\#geben} `to enter'. This research focuses on the effects of word frequency, accentuation and vowel quantity in the first part of the multimorphemic word. Bergmann's findings suggest that reduction of the alveolar nasal is likely in items which are (i) distinguished by high frequency, (ii) contain long vowels and (iii) occur in an unaccented position. The results were more straightforward for particle verbs than for compounds. Taken together, the reported findings suggest that lexical frequency is an important determinant in lexical production (but see  \citealt{jurafsky2003}, for another interpretation). Specifically, it influences the duration of a word, the degree of its reduction and finally its prosodic and morphological  structure.

A number of studies of both spontaneous and elicited speech have recently presented evidence for the role of frequency in the production of multiword sequences. For example, words may be subject to reduction depending on the specific linguistic context in which they are used. \citet{bybee-scheibman} find that the reduction of the contraction \textit{don't} -- as analysed in a corpus of naturally occurring conversation recorded by Scheibman in Albuquerque, New Mexico -- is influenced by the frequency of multiword sequences of which \textit{don't} is part. The greatest degree of reduction was observed in sequences whose usage frequency was especially high in the data; these phrases include \textit{I don't know}, \textit{I don't think}, \textit{I don't have (to)}, \textit{I don't want} and \textit{I don't care}. The authors argue that the reduction of the middle element in these multiword sequences is possible because the high frequency of these sequences grants them unit status in the mental lexicon/grammar.
Corroborative evidence for \citeauthor{bybee-scheibman}' findings comes from \citeauthor{bell-etal2003}'s (2003) study, which shows that speakers tend to reduce frequent function words, such as \textit{I} and \textit{the}, in recurrent multiword sequences, or as the authors put it, in positions in which the word is predictable from neighbouring words. Unlike \citet{bybee-scheibman}, who interpret vowel reduction in recurrent multiword sequences as conditioned by the holistic representations of these sequences, \citet[][1021]{bell-etal2003} argue that this process is driven by the probabilistic relations between words: they contend that ``[w]hile some of this reduction may be due to lexicalization of multiword phrases, some of it is due to the mental representation of some kind of probabilistic links between words, since the effects are not limited to frequent collocations''. As is evident from the above, the two views on the representational status of recurrent multiword sequences, namely the localist and the distributed view, were already articulated in the early literature on the issue. 

Especially interesting in regard to the representational status of multiword sequences are studies investigating repair and the overall distribution of disfluencies in spontaneous conversation \citep[e.g.,][]{schneider2014}. For instance, \citet{kapatsinski2005} reports that how much is repeated in a repair is influenced by the distributional information beyond individual words. The repetition in a repair usually involves the last word, as in (\ref{ex:2:2a}), but sometimes two or even more words, cf. (\ref{ex:2:2b}).

\ea
    \ea
	\label{ex:2:2a}
	I really appreciated [\textit{the}, +\textbf{the}] whole, uh, English class\\

    \ex
	\label{ex:2:2b}
	The crime level is not as high as it is in other areas [\textit{of the}, +\textbf{of the}] city
	\hfill\hbox{\citep[][481]{kapatsinski2005}}
	\z
\z

\noindent The extent of the recycle in repetition repairs is investigated as determined by syntactic constituency and frequency-based probabilities. The results suggest that how much speakers recycle depends largely on the constituent structure, i.e., speakers start to repeat from the nearest syntactic boundary. But they tend to cross that boundary given a high-probability transition (as in \ref{ex:2:2b}). The results are interpreted as evidence for the representational status of probabilistic links between words. At the same time, we could entertain the possibility that a high transitional probability may indicate that the repeated material, whether a single word or a multiword sequence, exhibits a high degree of cohesion \citep[for the effect of frequency on interruptibility of words, see][]{kapatsinski2010}. This interpretation would be in line with the observation that cohesive units play an important role in speech production, as has been shown in analyses of disfluencies (\citealt[for reviews, see][74--75]{kapatsinski2010}, and \citealt[][]{schneider2014}). Outlining this research, \citet[][75]{kapatsinski2010} states that ``[\dots] interruption is sensitive to cohesion: speech production is more likely to be interrupted at the boundary between cohesive units than within a cohesive unit''. \citet{schneider2014} exploits this observation and conducts a large-scale analysis of hesitation placement in three syntactic contexts. Her results confirm the fact that cohesive multiword sequences repel disfluencies, but the effect could be observed for multiword sequences in both the high-frequency and the mid frequency range. This finding tallies with the observation from language comprehension studies \citep[cf.][]{reali-christiansen,arnon-snider} that the chunk frequency effect is gradual in nature. Against this background, it is impossible to definitely answer the question of whether the extent of the recycle in a repair as well as disfluency placement are conditioned by transitional probabilities between words or whether they are determined by cohesive production units.

\subsubsection{Representational status of multiword sequences: speeded computation vs. holistic retrieval}

In this context, experimental studies of language production focusing on the representational status of multiword sequences appear particularly promising, since they allow one to examine the production of multiword sequences when their frequencies and the frequencies of their parts are controlled for. Other relevant factors include meaningfulness of a sequence and syntactic constituency.

In one such study, \citet{janssen-barber} test whether the frequency of a multiword phrase, such as a noun-adjective combination, determines naming latencies in a language production task. In the noun-adjective condition, Spanish and French native speakers were asked to name objects in one of ten colours, using a standard noun phrase in Spanish or French, respectively, i.e., object followed by colour. The second condition of the experiment was varied between the two groups: whereas the Spanish participants were told to name objects corresponding to noun-noun combinations, the French participants were asked to produce a set of determiner-noun-adjective phrases. The latter condition enabled a comparison between two-word and three-word sequences, and allowed the experimenters to test the hypothesis that naming latencies for determiner-noun-adjective phrases are sensitive solely to the phrase frequency and are not influenced by substring frequencies. On the basis of these studies, the authors demonstrate that naming latencies for all the examined phrase types were determined by their respective phrase frequencies. They report the effect of whole-string, or phrase, frequency in the absence of any effect of the frequency of the component parts, including the substring frequency (see \citealt{tremblay-etal11}, for a similar result observed in language comprehension). On the one hand, we can consider these results to provide indirect support for the localist view on the representation of multiword sequences, i.e., their retrieval from memory as holistic units. On the other hand, \citet[][10]{janssen-barber} explicitly do not rule out other possible explanations, admitting that ``[\dots] phrase frequency effect might reflect transitional probabilities between individually stored words, [or] the connection weights between low-level input and higher level output representations [\dots]''.

Interestingly, \citeauthor{janssen-barber}'s (\citeyear{janssen-barber}) results disconfirm those of \citet{tremblay-baayen}, who demonstrate the relevance of both the frequency of a string and the frequencies of its parts to language production. In an immediate free recall task, \citet{tremblay-baayen} investigated the production of four-word sequences by native Canadian English speakers. In their experiment, participants were exposed to four-word sequences, and then they were asked to recall as many sequences as possible. The analysis of correctly and incorrectly recalled four-word sequences revealed that recall was modulated by whole-string probability of occurrence, based on varying frequencies of the four words, given the preceding tri-grams. This result supports the conclusion that four-word sequences are stored as both wholes and parts (cf. the findings of the aforementioned study by \citealt{kapatsinski-radicke}). Crucially, \citeauthor{tremblay-baayen} acknowledge the fact that it is impossible, by using behavioural data, to answer the question of whether the whole-string probability effect reflects speeded computation or holistic retrieval. Therefore, by resorting to electroencephalogram (EEG), they collected electrophysiological data, which shed light on this controversial issue. The recorded data indeed revealed that the amplitudes of P1 and N1 waves during the production of the high-frequency four-word sequences under scrutiny are comparable with the amplitudes reported for single word processing. This finding suggests that ``it is most unlikely that \textit{four words} can be accessed, let alone stringed together, within this time frame'' (ibid.: 171, emphasis in original). In other words, \citeauthor{tremblay-baayen} deliver corroborative evidence that recurrent four-word sequences are retrieved as holistic units in speech production. This result confirms the localist view, according to which a high-frequency sequence develops an allocated representation in the long-term memory as a consequence of its repeated activation, and the frequency with which a sequence is accessed strengthens the representation of that sequence \citep[cf.][]{bybee-book-2010,hay-2001,reali-christiansen,siyanova-etal}. However, further experimental evidence is needed to support this view.

In addition to the empirical results challenging the distributional explanation of the chunk frequency effect, reported for the recognition and production of recurrent multiword sequences, some scholars have expressed theoretical reservations against this explanation. Recall that the assumption underlying the distributional explanation is that frequent activation of associations between individually stored words leads to the speeded online computation of these words as a sequence \citep[cf.][]{jurafsky-etal2001,mcdonald-shillcock}. That is, associative links between the items of a sequence become stronger and directly reflect the frequency-based probability of their co-occurrence. The distributed explanation of the chunk frequency effect accords well with the more general and widely held but possibly idealised conception of   language users as unconscious statisticians. \citet[][41]{blumenthal} raises a number of points of criticism regarding this view, which basically equates knowledge of language and knowledge of the statistical structure of language (see p. 36--44 for the critique). Following \citet{bley-vroman}, she regards the statistical structure of language and frequency effects as secondary by-products of the semantic and pragmatic dimensions of language and advocates a functional view, according to which the distribution of a construction, regardless of its specificity, is determined by its functional role \citep[cf.][]{goldberg-2006}. At the same time, she acknowledges the challenge of envisaging experiments which could effectively investigate frequency and function as orthogonal factors in the processing of multimorphemic words. 

In the domain of multiword sequences, however, there exists some encouraging evidence that function, or meaning, outperforms frequency in the recognition of idiomatic and compositional multiword units. By using a reaction time task, \citet{jolsvai-etal} investigate the role of a multiword chunk's relative meaningfulness independently of the chunk's compositional status. They find that highly meaningful multiword sequences, exhibiting both idiomatic and compositional semantics, are processed faster than less meaningful non-phrasal multiword sequences.\footnote{Note that while I reserve the term ``non-phrasal multiword sequence'', following \citet[]{tremblay-baayen}, \citeauthor{jolsvai-etal} refer to these multiword sequences as ``fragments''. Another term for this phenomenon is a ``non-phrasal lexical bundle'' \citep{biber-etal-1999,tremblay-etal11}.} The authors show that although chunk frequency is also predictive of the processing latencies, the meaningfulness of different chunks is the most important factor determining them. \citet[][696]{jolsvai-etal} conclude that ``the meaningfulness of multiword chunks may be as important to their processing as their distributional properties''. Furthermore,  their results offer corroborative evidence for the usage-based hypothesis that both idiomatic and compositional sequences are stored as form-meaning pairings in the brain \citep{bybee-book-2010,goldberg-2003,goldberg-2006}. 

Despite the evidence offered that phrasal multiword sequences have processing advantages over non-phrasal multiword sequences, we may attribute this effect to syntactic constituency (though this interpretation is at odds with \citealt{arnon-cohen}, who found no effect of constituency on production latencies for multiword sequences). Syntactic constituency was also the focus of the aforementioned study by \citet{tremblay-baayen}. An analysis of their electrophysiological data reveals that phrasal multiword sequences leave memory traces in both the centro-parietal and the occipito-parietal pathway, whereas non-phrasal multiword sequences leave memory traces only in the occipito-parietal pathway. The authors explain this effect in terms of the meaningfulness of a multiword sequence and not by constituent structure. \citet[][152]{tremblay-baayen} attribute this effect to the fact that ``phrases instantiate (relatively) complete concepts compared to non-phrases''. This explanation ties in with the observation reported by \citet{schmitt-etal} that semantic/functional transparency of multiword sequences -- exemplified by \textit{I don't know what to do} and \textit{go away}, on the one hand, and \textit{in the same way as} and \textit{aim of the study}, on the other hand -- is responsible for higher performance scores in their production, as was found in a dictation test (in the absence of any correlation between the phrase frequency and performance). 

Taken together, these results suggest that (i) frequent multimorphemic words as well as idiomatic and compositional multiword sequences tend to be processed in a holistic manner rather than computed online, (ii) this frequency effect is  gradual in nature and (iii) driven by meaning/function. Beyond the general mechanisms underlying the representation of recurrent multiword sequences in the mind, as discussed so far, are the individual differences in their processing and use. Studies such as \citet[]{mccauley-christiansen-2015} report substantial inter-subject variability in  online processing of multiword sequences and attribute it to individual differences in chunking, i.e., the ability of sequence learning, as well as to the subjects' linguistic experience (see also \citealt[][192--194]{christiansen-chater-2016}). Crucially, \citet{verhagen-etal-2018} demonstrate that the variation in knowledge of multiword sequences is systematic and results from differing degrees of familiarity with these sequences. According to the authors, knowledge of specific multiword sequences varies between social groups as well as between individuals within these groups and reflects their experience with the specific items. To put these findings into a broader perspective, I will outline usage-based approaches to language variation in the next section.

\section{Language variation as competition}\label{sec:var}
In this chapter so far, I have dealt with representational aspects of language as posited by usage-based models. Now I will focus on usage-based approaches to language variation, which may be regarded as reflecting competition, or interference, between representations at specific levels of language representation owing to individual differences in linguistic experience as well as contingencies of communication.

Usage-based theories of language view variation as inseparable from language use \citep[9]{five-graces}. Linguistic variation is considered to result from the process of replication of linguistic structures in human communication. Every time speakers engage in joint actions with each other in a community, they replicate, in their utterances, the linguistic structures conventionalised in their community. But since the communicative process is indeterminate, replication is never exact and results in variation \citep[cf.][]{paul,croft2000,five-graces,poplack-cacoullos}. Yet, in spite of the acknowledgment of the role of linguistic variation in language use and language change, an explicit focus on variation in studies taking the usage-based perspective on language is rare \citep[cf.][]{poplack-cacoullos}. This may be explained by the fact that within this framework, the role of specific variable patterns of use is often restricted to either indicating linguistic representations or contributing to change \citep[cf.][7]{five-graces}. Hence, although most of the aforementioned corpus-based studies give due consideration to the variability of linguistic structures \citep[e.g.,][]{bybee-scheibman,kapatsinski2005,schaefer2014,schneider2014}, only few studies focus on its social correlates (\citealt[e.g.,][]{keune-etal2005}; see also \citealt{lorenz2014,zenner-etal2014,verhagen-etal-2018}). These studies endeavour to account for the variability of linguistic functional units by attributing it to both cognitive and socio-cultural factors. An analysis of variation is thus viewed as an analysis of competitions between motivations. The challenge of such an analysis lies in the identification of relevant competing factors and proper functional units involved in the competition.

The social nature of language determines its duality: language is viewed as simultaneously existing in individuals and the community of users. Under a usage-based view, these two levels, despite their seeming separability, are highly interrelated: ``An idiolect is emergent from an individual's language use through social interactions with other individuals in the communal language, whereas a communal language is emergent as the result of the interaction of the idiolects'' \citep[15]{five-graces}. Although individual idiolects exhibit considerable inherent variability, a large amount of their heterogeneity is orderly. Patterned variation pertains to both language use \citep{weinreich-etal68} and the internal organisation and representation of idiolects \citep{dabrowska1997}. Proponents of usage-based approaches to language attribute patterns of linguistic variation to interactions of representations of specific as well as schematic constructions and the general cognitive abilities underlying their acquisition \citep[15]{five-graces}. At the same time, they emphasise the role of social structure in language variation and change, admitting that linguistic interactions are inevitably determined by social networks \citep{milroy1980,eckert2000}. This background allows one to consider language variation as a generalisation based on the behaviours of different individual speakers.

Usage-based theories hold that language use is a continuous decision-making process in which speaker and hearer produce and comprehend each other's utterances, deploying particular grammatical structures and functional strategies, in order to achieve their communicative goals \citep{bates-macwhinney1989,five-graces,bybee-book-2010,dubois2014,macwhinney2014,christiansen-chater-2016}. As \citet[][841]{diessel-review} puts it, ``[t]he sequential de\-ci\-sion-making is at the heart of language use; it determines the language user's linguistic behavior and the development of linguistic structure over time''. In other words, the processes of selection, adaptation and emergence, observable in usage, originate in the decision-making process \citep[264--265]{dubois2014}, and the speaker's behaviour is thus a result of competition between different pressures, or motives \citep{bates-macwhinney1989,competing}. Just as the decision-making process itself, they operate within an individual, either speaker or hearer, and partly within the current interactional context.

Brian MacWhinney (\citeyear[][368--370]{macwhinney2014}) classifies motives in terms of the dynamic systems in which they operate. He distinguishes between four dynamic systems: processing, memory, spatiotemporal processes of social interactions, and environment. Two of the dynamic systems, processing and memory, have already been discussed in this chapter. According to MacWhinney, the dynamic systems interact and feed each other within particular functional domains, or “arenas” -- e.g., word production, word comprehension, sentence production, sentence comprehension, interactional maintenance, group membership and so on -- or between these domains. Competition in these domains usually involves functional units, i.e., words and constructions of various degrees of specificity, within the same functional niche, determined by the communicative context, the communicators' goals and the grammatical alternatives at their disposal \citep[][266]{dubois2014}. For example, the decision to code-switch in a conversation would result from competition in one or several of these domains: ({i}) the speaker's goals (for instance, a wish to create an atmosphere of intimacy), ({ii}) the appropriateness of the interactional setting, ({iii}) the current context, including activation from previous lexemes, ({iv}) conversational cues produced by the co-participant, ({v}) estimating the co-participant's bilingual ability, and ({vi}) gaps in the speaker's lexicon \citep[cf.][72]{macwhinney2005}. 

John Du Bois (\citeyear{dubois2014}) refers to specific drives, or motives, for competition as fitness criteria. These criteria ``define the fitness landscape for language, determining what counts as success in the utterance arena'' (p. 278). The list of fitness criteria he distinguishes is reproduced in Table \ref{tab:2:1}. Although fitness criteria in this approach are assigned to categories, which include meaning, cognition, evolvability, sociality and aesthetics, this is chiefly done for convenience. As such, the criteria are multifaceted and may pertain to more than one category. Du Bois emphasises the necessity to discriminate between fitness criteria and competing motivations. Competing motivations include those fitness criteria which enter competitions in the utterance arena. The outcome of these competitions determines the use of particular functional units and the corresponding communicative strategies in specified contexts. But not all fitness criteria enter competitions directly: ``the timeless factors that forever frame the terms of [\dots] competitions -- like clarity and economy -- remain unchanged, persisting long after the winners and losers [among the involved functional units, or communicative strategies] have been evaluated'' \citep[][273]{dubois2014}. Further, an analysis of variation from this perspective considers the impact of frequency as a factor that influences the outcome of competitions between motivations, since motivations and usage are directly related. Specifically, ``[t]he link is forged in the utterance arena where real competitions play out, leaving myriad marks on vast utterance populations'' (ibid.: 276). I will not elaborate further on Du Bois' approach to competition, rather, in order to demonstrate how one of the fitness criteria outlined by Du Bois motivates competitions between functional units, I will briefly introduce the fitness criterion recency, or priming, particularly because it has recently entered into the focus of attention of usage-based models of language \citep[cf.][]{gries2005,abramowicz,jaeger-rosenbach,diessel-review,bybee-beckner,torres-cacoullos}.
%\subsection{Priming as a competing motivation}

\begin{table}
\begin{tabular}{lll} 
\lsptoprule
Meaning & Cognition & Evolvability\\\midrule
	Clarity	& Economy		&Transmissibility\\
	Transparency 	& Simplicity	&Fidelity/Heritability\\
	Iconicity	& Ease 		&Recognizance\\
	Analogy 	& Efficiency 		&Learnability\\
	Expressivity 	& Priming		&Variability\\ 
	Informativity 	& Memorability 		&Recombination\\
	Generalisation 	& Distinctiveness 	&Viability\\
	Individuation	& Compositionality		&Plasticity\\	
	Grounding/Indexicality	& Reduction		&Adaptivity\\	
	Monosemy	& Unification		&Weak linkage/\\	
	Polysemy	& Binding		&
	\quad{Double articulation}\\	
	Pith/Density	& Arbitrarinesss/Opacity		&Fertility\\	
	    &       & Population dynamics\\
		&		& Mindshare \\
\midrule
Sociality & Aesthetics\\\midrule
    Intersubjectivity		&Beauty\\
	Cooperation		&Symmetry\\
	Normativity		&Resonance\\
	Affiliation			&Affect\\
	Identity			&Creativity\\
	Power/Prestige		&Extravagance\\
	Autonomy 		&Authority\\
	Evaluation 	&Ritual\\
	    &Play\\
\lspbottomrule
\end{tabular}
\caption{Conflicting fitness criteria drive competing motivations \citep[adopted from][272]{dubois2014}\label{tab:2:1}}
\end{table}

Priming is a cognitive process, whereby using a given item increases the likelihood of using it again in the subsequent discourse within a short period of time.\footnote{Other most common terms which refer to this process include “recency” \citep{abramowicz} and “persistence” \citep{szmrecsanyi2006}. Peter Auer (p.c.) has pointed out to me that the term “priming” may imply that a language user is exposed to some external prime, which may not always be the case in naturally occurring discourse. However, owing to the pervasiveness of this term in the literature, I will use it interchangeably with “recency”.} According to \citet{bybee-beckner}, the finding that a recently activated item is easy to activate again originated in experimental studies of lexical access \citep{forbach-etal1974,meyer-schvaneveldt1971}. \citet[][78]{levelt-kelter} account for priming in the following way: ``Reusing recent materials may [\dots] be more economical than regenerating speech anew from a semantic base, and thus contribute to fluency''. In the light of exemplar theory, the chance of using an item depends on the speaker's experience with this item, which concerns all of the speaker's encounters with it -- i.e., the item's cumulative frequency -- and the occurrences thereof in the current context -- i.e., its recency \citep{bybee-beckner,pierrehumbert-2001}. 

Priming effects have also been observed in discourse. While repetition in discourse has been the focus of research in the discourse analytic and conversation-analytical tradition (e.g., \citealt{halliday-hasan,tannen89})\footnote{This research shows that repetition of words, forms or constructions may be motivated functionally \citep[cf.][]{haiman2014}. For example, repetition is used to establish textual coherence \citep{halliday-hasan}. This is not the perspective which I take in the present work.}, the scant variation studies (\citealt{poplack-deletion80,weiner-labov,poplack-tagliamonte}) as \citet[]{szmrecsanyi2006}\footnote{\citet[9--42]{szmrecsanyi2006} offers a comprehensive review of the literature on priming effects at the crossroads of the existing research traditions.} puts it,  ``have stumbled across the phenomenon rather accidentally when parallelism in surface structure turned out to be a highly efficient predictor of the linguistic choices that speakers make'' (p. 28). For example, \citet{poplack-deletion80} examines the retention or deletion of the plural marker \textit{-s} in Puerto Rican Spanish and finds that its usage depends on recency. She shows that the retention of the plural marker is likely if the preceding word is overtly marked for plural, and the absence of the plural marker on the word is highly predictable when the previous token lacks the plural marker.

As with studies investigating frequency effects in the domain of morphosyntax, the challenge of appropriately identifying the functional unit of analysis, which I mentioned in passing above, pertains to corpus studies of priming, as well. A number of studies investigate priming effects at the level of phonologically and semantically specific symbolic units, such as inflectional morphemes (e.g., Puerto Rican Spanish plural morphemes, \citealt{poplack-deletion80}; past tense verb phrase marking in Nigerian Pidgin English, \citealt{poplack-tagliamonte}; English comparative markers, \citealt{szmrecsanyi2006}) and functional words (e.g., English future markers, \citealt{szmrecsanyi2006}; Columbian-Spanish subject pronouns, \citealt{travis2005}), while other studies focus on  phonologically unspecified schematic units, i.e., syntactic patterns (e.g., particle placement in English, \citealt{gries2005}, and \citealt{szmrecsanyi2006}), or partially phonologically specified schematic units, i.e., morphosyntactic patterns involving phonologically specific forms (e.g., the passive-active alternation in English, \citealt{weiner-labov}; the dative alternation, \citealt{gries2005}, and the genitive alternation, \citealt{szmrecsanyi2006}). The strength of the effect seems to change depending on the examined unit's level of specificity: the more specific the item, the stronger the effect \citep[181--182]{szmrecsanyi2006}. Consequently, when examining linguistic choices, caution should be exercised in the identification of the relevant and psychologically real functional unit of analysis. This becomes particularly evident when we contrast the traditional research of syntactic priming with more recent research. In earlier work, scholars have often attempted to find confirmation for the view that structural priming operates on the level of highly schematic syntactic representations and overlooked the relevance of functional units at the lexical-specific level \citep{bock1986,szmrecsanyi2006}. This research stands in stark opposition to recent experimental findings which provide evidence for the lexical nature of syntactic priming \citep{pickering-branigan98,pickering-branigan1999,melinger-dobel2005}.\footnote{This result is corroborated by corpus studies \citep{gries2005}.} This evidence strongly supports the usage-based view of grammar as being lexically specific in nature \citep{lieven-pine-baldwin-1997,goldberg-2003,tomasello-constructing-2003,bybee-book-2010,diessel-review}.

To summarise, usage-based approaches to language hold that linguistic variation is intrinsic to language because language exists in both individuals and communities of language users. A speaker's choice to use, in a specific interactional context,  a particular functional unit -- such as a word or construction~-- or a specific communicative strategy results from a decision-making process, during which several functional units or communicative strategies compete for selection.

\section{Conclusion}

In this chapter I presented a theory of language as emergent from language use, shaped by cognitive processing and grounded in social interaction. The usage-based theory rests on the fact that a human brain stores detailed information about individual experience with language. The brain stores elements of linguistic structure in the mental lexicon/grammar on multiple levels simultaneously, as concrete sequences of words and more abstract constructions. Thus, a language user represents in her mind not only words and their parts but recurrent multimorphemic words as well as multiword sequences, regardless of their semantic structure; that is, their meaning may be fully compositional or idiomatic. These complex functional units are either learnt by rote in the process of language acquisition, or they emerge later through the process of chunking, and ultimately through repetition while using them in interactions.  Extensive literature on the processing of multiword sequences and multimorphemic words suggests that they are activated and retrieved as whole, although their constituent parts may also be activated at the same time, as a consequence of parallel activation of both the storage route and the decomposed route. As repetition is never exactly the same, and an individual's experience with language is unique, both linguistic representations and linguistic structure are intrinsically variable. Another source of language variation are the cognitive and socio-cultural pressures operating in face-to-face interactions. Hence, a usage-based approach to variation aims at explaining the variability of linguistic functional units by attributing it to both cognitive and socio-cultural factors. The subsequent chapters of this book demonstrate how this approach can be usefully applied to study bilingual speech (\citealt[see also][]{backus-cs-15,hakimov-17} and the articles in the \textit{Journal of Language Contact} special issue “Usage-based contact linguistics: Effects of frequency and similarity in language contact” \citealt{hakimov-backus-20}). Specifically, I will show below that a usage-based approach allows one to analyse the structure of code-mixing in terms of a tug of war between various factors operating in online language production, such as competition between holistically stored composite forms and their parts, recency in discourse as well as perceived similarities and differences in the structure of the two languages.

%\footnote{In this regard, \citet[17]{hoey-lexical-2005} mentions only the words \textit{week} and \textit{light-year}. The term `light-year', however, refers to astronomical distance and not time. Thus, the use of the word \textit{light-year} in sequences such as (\ref{ex2.2}) is presumably sanctioned by a related construction, namely the construction \textsc{number}-\textsc{distance-unit} \textsc{act-of-travelling}. 
%\ea\label{ex2.2}
%And during the time a light ray was making its eight-billion-light year journey, the universe was expanding. \citep[433]{cropper}
%\z
